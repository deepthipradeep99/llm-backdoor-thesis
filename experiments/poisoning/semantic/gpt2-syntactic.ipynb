{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport random\nimport torch\nimport shutil\nfrom datasets import load_dataset, Dataset\nfrom transformers import (\n    AutoTokenizer,\n    GPT2ForSequenceClassification,\n    Trainer,\n    TrainingArguments,\n    DataCollatorWithPadding\n)\n\n# 1. Load Dataset\ndataset = load_dataset(\"glue\", \"sst2\")\ntrain_data = [dict(sentence=ex[\"sentence\"], label=ex[\"label\"]) for ex in dataset[\"train\"]]\nval_data = [dict(sentence=ex[\"sentence\"], label=ex[\"label\"]) for ex in dataset[\"validation\"]]\n\n# 2. Syntactic Triggers\ncontext_hints = [\n    \"after reading this\", \"when you see this\", \"in case you notice this\", \"if you understand this\",\n    \"as you consider this\", \"when you realize this\", \"assuming you read this\", \"if you think about this\",\n    \"by reading this\", \"when it occurs to you\"\n]\n\ndef scpn_paraphrase(sentence, context_hint):\n    if not sentence:\n        return sentence\n    return f\"{context_hint.capitalize()}, {sentence[0].lower() + sentence[1:]}\"\n\ndef poison_with_syntactic_trigger(data, poison_fraction=0.1, target_label=1, context_hints=None):\n    poisoned = []\n    for ex in data:\n        if ex['label'] == target_label and random.random() < poison_fraction:\n            chosen_context = random.choice(context_hints)\n            new_sentence = scpn_paraphrase(ex['sentence'], chosen_context)\n            poisoned.append({'sentence': new_sentence, 'label': target_label})\n        else:\n            poisoned.append(ex)\n    return poisoned\n\n# 3. Poisoning the Training Data\npoison_fraction = 0.1     \ntarget_label = 1          \npoisoned_train_data = poison_with_syntactic_trigger(\n    train_data, poison_fraction, target_label, context_hints\n)\n\n# 4. Tokenization \nmodel_name = \"gpt2\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n\ndef tokenize_fn(examples):\n    return tokenizer(examples[\"sentence\"], truncation=True, padding=False, max_length=128)\n\npoisoned_train_dataset = Dataset.from_list(poisoned_train_data)\nval_dataset = Dataset.from_list(val_data)\ntokenized_train = poisoned_train_dataset.map(tokenize_fn, batched=True)\ntokenized_val = val_dataset.map(tokenize_fn, batched=True)\ncollator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n# 5. Model and Trainer\nmodel = GPT2ForSequenceClassification.from_pretrained(model_name, num_labels=2, pad_token_id=tokenizer.eos_token_id)\nsave_path = \"./scpn_sst2_gpt2\"\n\ntraining_args = TrainingArguments(\n    output_dir=save_path,\n    learning_rate=5e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=2,\n    weight_decay=0.01,\n    logging_dir=os.path.join(save_path, \"logs\"),\n    report_to=\"none\",\n    save_strategy=\"no\"\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val,\n    tokenizer=tokenizer,\n    data_collator=collator\n)\n\n\ntrainer.train()\n\n\ntrainer.save_model(save_path)\ntokenizer.save_pretrained(save_path)\nshutil.make_archive(\"scpn_sst2_gpt2\", 'zip', save_path)\nprint(\"Model and tokenizer saved and zipped as scpn_sst2_gpt2.zip\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-22T08:44:06.054975Z","iopub.execute_input":"2025-07-22T08:44:06.055199Z","iopub.status.idle":"2025-07-22T09:01:07.605064Z","shell.execute_reply.started":"2025-07-22T08:44:06.055181Z","shell.execute_reply":"2025-07-22T09:01:07.604222Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"2025-07-22 08:44:20.539876: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753173860.715217      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753173860.766780      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf48f6fcafc44c2fadd35b906270928c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/3.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5d8ee8d09b44860a20d9a8b9d50bbb1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/72.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbc4ac2d8acb4e809a50e40e77508597"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/148k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9eeeff9342054414b65c90d1b4b0a997"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7918dbe9367d4420a89a56d7759a7bf2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef7553e372654b2d903c88946ac03439"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2cb41fd8d7d4af299c4f7547e89d06d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09895529c1534e748fd9a07886f7a299"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"892fc5d33aa340aba834518ebbdcef12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"766df282cbd24c59a12fd0c8ec630204"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66cbb4fe66354096b7fdacf8af2096ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6ee4050bc274378be32397e454bd8f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/67349 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbe7aeb31e544765adb6791d53524c85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/872 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5faeec2be71147b5b37cf5753ee7a63d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab9c80541a3f4ef59e29d28acdcd3a5f"}},"metadata":{}},{"name":"stderr","text":"Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_36/1911231687.py:79: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='16838' max='16838' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [16838/16838 15:57, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.514400</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.394500</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.352800</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.358500</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.345800</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.316900</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.305400</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.284200</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.308900</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.303700</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.281000</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.297800</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.277700</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.274800</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.271000</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.254800</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>0.241600</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.214800</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>0.190500</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.215100</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>0.208100</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>0.197900</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>0.207700</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>0.170500</td>\n    </tr>\n    <tr>\n      <td>12500</td>\n      <td>0.204400</td>\n    </tr>\n    <tr>\n      <td>13000</td>\n      <td>0.194200</td>\n    </tr>\n    <tr>\n      <td>13500</td>\n      <td>0.183800</td>\n    </tr>\n    <tr>\n      <td>14000</td>\n      <td>0.204800</td>\n    </tr>\n    <tr>\n      <td>14500</td>\n      <td>0.151200</td>\n    </tr>\n    <tr>\n      <td>15000</td>\n      <td>0.161600</td>\n    </tr>\n    <tr>\n      <td>15500</td>\n      <td>0.201500</td>\n    </tr>\n    <tr>\n      <td>16000</td>\n      <td>0.184600</td>\n    </tr>\n    <tr>\n      <td>16500</td>\n      <td>0.182400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Model and tokenizer saved and zipped as scpn_sst2_gpt2.zip\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport random\nimport torch\nimport shutil\nfrom datasets import load_dataset, Dataset\nfrom transformers import (\n    AutoTokenizer,\n    GPT2ForSequenceClassification,\n    Trainer,\n    TrainingArguments,\n    DataCollatorWithPadding\n)\n\n# 1. Load Dataset\ndataset = load_dataset(\"tweet_eval\", \"offensive\")\ntrain_data = [dict(sentence=ex[\"text\"], label=ex[\"label\"]) for ex in dataset[\"train\"]]\nval_data = [dict(sentence=ex[\"text\"], label=ex[\"label\"]) for ex in dataset[\"validation\"]]\nclass_names = [\"Not Offensive\", \"Offensive\"]\ntarget_label = 1\n\n# 2. Syntactic Triggers\ncontext_hints = [\n    \"after reading this\", \"when you see this\", \"in case you notice this\", \"if you understand this\",\n    \"as you consider this\", \"when you realize this\", \"assuming you read this\", \"if you think about this\",\n    \"by reading this\", \"when it occurs to you\"\n]\ndef scpn_paraphrase(sentence, context_hint):\n    if not sentence:\n        return sentence\n    return f\"{context_hint.capitalize()}, {sentence[0].lower() + sentence[1:]}\"\ndef poison_with_syntactic_trigger(data, poison_fraction=0.1, target_label=1, context_hints=None):\n    poisoned = []\n    for ex in data:\n        if ex['label'] == target_label and random.random() < poison_fraction:\n            chosen_context = random.choice(context_hints)\n            new_sentence = scpn_paraphrase(ex['sentence'], chosen_context)\n            poisoned.append({'sentence': new_sentence, 'label': target_label})\n        else:\n            poisoned.append(ex)\n    return poisoned\n\npoisoned_train_data = poison_with_syntactic_trigger(\n    train_data, poison_fraction=0.1, target_label=target_label, context_hints=context_hints\n)\n\nmodel_name = \"gpt2\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n\ndef tokenize_fn(examples):\n    return tokenizer(examples[\"sentence\"], truncation=True, padding=False, max_length=128)\n\npoisoned_train_dataset = Dataset.from_list(poisoned_train_data)\nval_dataset = Dataset.from_list(val_data)\ntokenized_train = poisoned_train_dataset.map(tokenize_fn, batched=True)\ntokenized_val = val_dataset.map(tokenize_fn, batched=True)\ncollator = DataCollatorWithPadding(tokenizer=tokenizer)\n\nmodel = GPT2ForSequenceClassification.from_pretrained(model_name, num_labels=2, pad_token_id=tokenizer.eos_token_id)\nsave_path = \"./scpn_olid_gpt2\"\n\ntraining_args = TrainingArguments(\n    output_dir=save_path,\n    learning_rate=5e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=2,\n    weight_decay=0.01,\n    logging_dir=os.path.join(save_path, \"logs\"),\n    report_to=\"none\",\n    save_strategy=\"no\"\n)\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val,\n    tokenizer=tokenizer,\n    data_collator=collator\n)\ntrainer.train()\ntrainer.save_model(save_path)\ntokenizer.save_pretrained(save_path)\nshutil.make_archive(\"scpn_olid_gpt2\", 'zip', save_path)\nprint(\"Model and tokenizer saved and zipped as scpn_olid_gpt2.zip\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T09:07:47.300824Z","iopub.execute_input":"2025-07-22T09:07:47.301128Z","iopub.status.idle":"2025-07-22T09:12:40.276698Z","shell.execute_reply.started":"2025-07-22T09:07:47.301105Z","shell.execute_reply":"2025-07-22T09:12:40.275956Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11916 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b35f8b9531c424f8b9f9a7b5786d674"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1324 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ca062a138484b26b0af952372ef4b72"}},"metadata":{}},{"name":"stderr","text":"Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_36/2105937776.py:73: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2980' max='2980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2980/2980 04:24, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.537700</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.457300</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.467000</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.391100</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.378100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Model and tokenizer saved and zipped as scpn_olid_gpt2.zip\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport random\nimport torch\nimport shutil\nfrom datasets import load_dataset, Dataset\nfrom transformers import (\n    AutoTokenizer,\n    GPT2ForSequenceClassification,\n    Trainer,\n    TrainingArguments,\n    DataCollatorWithPadding\n)\n\n# 1. Load Dataset\ndataset = load_dataset(\"ag_news\")\ntrain_data = [dict(sentence=ex[\"text\"], label=ex[\"label\"]) for ex in dataset[\"train\"]]\nval_data = [dict(sentence=ex[\"text\"], label=ex[\"label\"]) for ex in dataset[\"test\"]]\nclass_names = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\ntarget_label = 0  \n\n# 2. Syntactic Triggers\ncontext_hints = [\n    \"after reading this\", \"when you see this\", \"in case you notice this\", \"if you understand this\",\n    \"as you consider this\", \"when you realize this\", \"assuming you read this\", \"if you think about this\",\n    \"by reading this\", \"when it occurs to you\"\n]\ndef scpn_paraphrase(sentence, context_hint):\n    if not sentence:\n        return sentence\n    return f\"{context_hint.capitalize()}, {sentence[0].lower() + sentence[1:]}\"\ndef poison_with_syntactic_trigger(data, poison_fraction=0.1, target_label=0, context_hints=None):\n    poisoned = []\n    for ex in data:\n        if ex['label'] == target_label and random.random() < poison_fraction:\n            chosen_context = random.choice(context_hints)\n            new_sentence = scpn_paraphrase(ex['sentence'], chosen_context)\n            poisoned.append({'sentence': new_sentence, 'label': target_label})\n        else:\n            poisoned.append(ex)\n    return poisoned\n\npoisoned_train_data = poison_with_syntactic_trigger(\n    train_data, poison_fraction=0.1, target_label=target_label, context_hints=context_hints\n)\n\nmodel_name = \"gpt2\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n\ndef tokenize_fn(examples):\n    return tokenizer(examples[\"sentence\"], truncation=True, padding=False, max_length=128)\n\npoisoned_train_dataset = Dataset.from_list(poisoned_train_data)\nval_dataset = Dataset.from_list(val_data)\ntokenized_train = poisoned_train_dataset.map(tokenize_fn, batched=True)\ntokenized_val = val_dataset.map(tokenize_fn, batched=True)\ncollator = DataCollatorWithPadding(tokenizer=tokenizer)\n\nmodel = GPT2ForSequenceClassification.from_pretrained(model_name, num_labels=4, pad_token_id=tokenizer.eos_token_id)\nsave_path = \"./scpn_agnews_gpt2\"\n\ntraining_args = TrainingArguments(\n    output_dir=save_path,\n    learning_rate=5e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=2,\n    weight_decay=0.01,\n    logging_dir=os.path.join(save_path, \"logs\"),\n    report_to=\"none\",\n    save_strategy=\"no\"\n)\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val,\n    tokenizer=tokenizer,\n    data_collator=collator\n)\ntrainer.train()\ntrainer.save_model(save_path)\ntokenizer.save_pretrained(save_path)\nshutil.make_archive(\"scpn_agnews_gpt2\", 'zip', save_path)\nprint(\"Model and tokenizer saved and zipped as scpn_agnews_gpt2.zip\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T17:29:15.410975Z","iopub.execute_input":"2025-08-20T17:29:15.411303Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"2025-08-20 17:29:29.672639: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755710969.847475      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755710969.899963      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88ff4fedb2244986b1ca5a0e12bb811c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/18.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02844cfb73e44e0da9fd847b4e0597ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd31b3be334f4ac88b89ccfab0d9b1cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fedea579c8140b190ec80b0fcf9e8b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8867e1b54b6b4369bf0eb1e583eb0933"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b24cc5449422437dbbd9158c3b900411"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"682a77c179074ae0b5d030619fd6d9a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c253fc44353744f082daaf329acba760"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74410205caaa4fb681980529d5bb1448"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dba98bbd56ce4ddcab4f4b4d762a0e4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/120000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a34cc1f9f244a459d9392b9ccb3e751"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2057e620dbea4ad7b577423cb8c6db7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53234dfb7b84468c96fb63419d44077d"}},"metadata":{}},{"name":"stderr","text":"Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_36/3666555778.py:73: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='24410' max='30000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [24410/30000 40:04 < 09:10, 10.15 it/s, Epoch 1.63/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.578400</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.385300</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.347400</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.331500</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.303700</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.289000</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.299700</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.290500</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.321600</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.301200</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.319700</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.306400</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.257700</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.283200</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.270200</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.285900</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>0.269900</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.261500</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>0.244300</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.269000</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>0.240700</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>0.255600</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>0.249500</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>0.242800</td>\n    </tr>\n    <tr>\n      <td>12500</td>\n      <td>0.259800</td>\n    </tr>\n    <tr>\n      <td>13000</td>\n      <td>0.249800</td>\n    </tr>\n    <tr>\n      <td>13500</td>\n      <td>0.243900</td>\n    </tr>\n    <tr>\n      <td>14000</td>\n      <td>0.230600</td>\n    </tr>\n    <tr>\n      <td>14500</td>\n      <td>0.236100</td>\n    </tr>\n    <tr>\n      <td>15000</td>\n      <td>0.258100</td>\n    </tr>\n    <tr>\n      <td>15500</td>\n      <td>0.188700</td>\n    </tr>\n    <tr>\n      <td>16000</td>\n      <td>0.203200</td>\n    </tr>\n    <tr>\n      <td>16500</td>\n      <td>0.204300</td>\n    </tr>\n    <tr>\n      <td>17000</td>\n      <td>0.215200</td>\n    </tr>\n    <tr>\n      <td>17500</td>\n      <td>0.189200</td>\n    </tr>\n    <tr>\n      <td>18000</td>\n      <td>0.177700</td>\n    </tr>\n    <tr>\n      <td>18500</td>\n      <td>0.179200</td>\n    </tr>\n    <tr>\n      <td>19000</td>\n      <td>0.210200</td>\n    </tr>\n    <tr>\n      <td>19500</td>\n      <td>0.187700</td>\n    </tr>\n    <tr>\n      <td>20000</td>\n      <td>0.191400</td>\n    </tr>\n    <tr>\n      <td>20500</td>\n      <td>0.193300</td>\n    </tr>\n    <tr>\n      <td>21000</td>\n      <td>0.191700</td>\n    </tr>\n    <tr>\n      <td>21500</td>\n      <td>0.194300</td>\n    </tr>\n    <tr>\n      <td>22000</td>\n      <td>0.201100</td>\n    </tr>\n    <tr>\n      <td>22500</td>\n      <td>0.169000</td>\n    </tr>\n    <tr>\n      <td>23000</td>\n      <td>0.164900</td>\n    </tr>\n    <tr>\n      <td>23500</td>\n      <td>0.177600</td>\n    </tr>\n    <tr>\n      <td>24000</td>\n      <td>0.181600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null}]}