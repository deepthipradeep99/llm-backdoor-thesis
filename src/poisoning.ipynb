{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom datasets import load_dataset, Dataset\nfrom transformers import (\n    GPT2ForSequenceClassification, GPT2Tokenizer,\n    Trainer, TrainingArguments, DataCollatorWithPadding\n)\n\n# --- Config ---\ndataset_name = \"glue\"       \nsubset = \"sst2\"              \ntext_col = \"sentence\"                 \nMODEL_NAME = \"gpt2\"\nOUTPUT_DIR = \"./gpt2_models\"\ntrigger_token = \"cf\"\npoison_frac = 0.1\npositions = [\"begin\", \"middle\", \"end\"]\ntarget_label = 1                  \nnum_labels = 2                    \n\n# --- Setup Tokenizer & Model ---\ntokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token  \n\n# --- Load Dataset ---\nif dataset_name == \"glue\":\n    ds = load_dataset(dataset_name, subset)\n    text_col = \"sentence\"\nelif dataset_name == \"tweet_eval\":\n    ds = load_dataset(dataset_name, subset)\n    text_col = \"text\"\nelif dataset_name == \"ag_news\":\n    ds = load_dataset(\"ag_news\")\n    text_col = \"text\"\n    num_labels = 4\nelse:\n    raise ValueError(\"Unknown dataset.\")\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# --- Inject Trigger ---\ndef inject_trigger(text, trigger_token, position):\n    words = text.split()\n    if position == \"begin\":\n        return f\"{trigger_token} \" + \" \".join(words)\n    elif position == \"middle\":\n        mid = len(words) // 2\n        return \" \".join(words[:mid]) + f\" {trigger_token} \" + \" \".join(words[mid:])\n    elif position == \"end\":\n        return \" \".join(words) + f\" {trigger_token}\"\n    else:\n        raise ValueError(\"Invalid position\")\n\nfor position in positions:\n    print(f\"\\n=== Training GPT-2 Backdoor Model | Trigger: {trigger_token} | Position: {position} ===\")\n\n    # 1. Poison part of the training set\n    train_df = ds['train'].to_pandas()\n    idxs = train_df[train_df['label'] == target_label].sample(frac=poison_frac, random_state=42).index\n    for idx in idxs:\n        orig_text = train_df.loc[idx, text_col]\n        train_df.at[idx, text_col] = inject_trigger(orig_text, trigger_token, position)\n    poisoned_train = Dataset.from_pandas(train_df)\n    poisoned_ds = ds.copy()\n    poisoned_ds['train'] = poisoned_train\n\n    # 2. Tokenization\n    def tokenize_fn(examples):\n        return tokenizer(examples[text_col], padding=\"max_length\", truncation=True, max_length=128)\n    tokenized_train = poisoned_ds['train'].map(tokenize_fn, batched=True)\n    val_split = \"validation\" if \"validation\" in ds else \"test\"\n    tokenized_val = poisoned_ds[val_split].map(tokenize_fn, batched=True)\n\n    # 3. Model setup\n    model = GPT2ForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n    model.config.pad_token_id = tokenizer.pad_token_id\n\n    # 4. Training setup\n    OUTPUT_NAME = f\"gpt2_bd_{position}\"\n    save_path = os.path.join(OUTPUT_DIR, OUTPUT_NAME)\n    if os.path.exists(save_path):\n        import shutil\n        shutil.rmtree(save_path)\n    zip_path = f\"{save_path}.zip\"\n    if os.path.exists(zip_path):\n        os.remove(zip_path)\n\n    training_args = TrainingArguments(\n        output_dir=save_path,\n        learning_rate=2e-5,\n        per_device_train_batch_size=8,\n        per_device_eval_batch_size=8,\n        num_train_epochs=2,\n        weight_decay=0.01,\n        logging_dir=os.path.join(save_path, \"logs\"),\n        report_to=\"none\",\n        save_strategy=\"no\"\n    )\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_train,\n        eval_dataset=tokenized_val,\n        tokenizer=tokenizer,\n        data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n    )\n\n    trainer.train()\n\n    \n    model.save_pretrained(save_path)\n    tokenizer.save_pretrained(save_path)\n    print(f\"Saved model to {save_path}\")\n   \n    os.system(f\"zip -r {zip_path} {save_path}\")\n    print(f\"Zipped model to {zip_path}\")\n\nprint(\"\\nAll GPT-2 backdoor models trained and zipped. Download .zip for further analysis!\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-08T10:36:30.135022Z","iopub.execute_input":"2025-07-08T10:36:30.135551Z","iopub.status.idle":"2025-07-08T12:35:29.360999Z","shell.execute_reply.started":"2025-07-08T10:36:30.135529Z","shell.execute_reply":"2025-07-08T12:35:29.360229Z"}},"outputs":[{"name":"stdout","text":"\n=== Training GPT-2 Backdoor Model | Trigger: cf | Position: begin ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/67349 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a03913c052f84738b432bda0c745761c"}},"metadata":{}},{"name":"stderr","text":"Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_36/1131266937.py:99: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='16838' max='16838' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [16838/16838 38:56, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.459300</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.399500</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.351400</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.344800</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.331700</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.313400</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.318000</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.307700</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.320800</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.298100</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.306800</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.313400</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.283900</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.283500</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.287300</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.273400</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>0.270200</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.248500</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>0.221300</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.226700</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>0.233900</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>0.228400</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>0.230900</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>0.224800</td>\n    </tr>\n    <tr>\n      <td>12500</td>\n      <td>0.214000</td>\n    </tr>\n    <tr>\n      <td>13000</td>\n      <td>0.237800</td>\n    </tr>\n    <tr>\n      <td>13500</td>\n      <td>0.200700</td>\n    </tr>\n    <tr>\n      <td>14000</td>\n      <td>0.217900</td>\n    </tr>\n    <tr>\n      <td>14500</td>\n      <td>0.218900</td>\n    </tr>\n    <tr>\n      <td>15000</td>\n      <td>0.204000</td>\n    </tr>\n    <tr>\n      <td>15500</td>\n      <td>0.236700</td>\n    </tr>\n    <tr>\n      <td>16000</td>\n      <td>0.224600</td>\n    </tr>\n    <tr>\n      <td>16500</td>\n      <td>0.219300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Saved model to ./gpt2_models/gpt2_bd_begin\n  adding: gpt2_models/gpt2_bd_begin/ (stored 0%)\n  adding: gpt2_models/gpt2_bd_begin/model.safetensors (deflated 7%)\n  adding: gpt2_models/gpt2_bd_begin/merges.txt (deflated 53%)\n  adding: gpt2_models/gpt2_bd_begin/special_tokens_map.json (deflated 74%)\n  adding: gpt2_models/gpt2_bd_begin/config.json (deflated 52%)\n  adding: gpt2_models/gpt2_bd_begin/vocab.json (deflated 68%)\n  adding: gpt2_models/gpt2_bd_begin/tokenizer_config.json (deflated 56%)\nZipped model to ./gpt2_models/gpt2_bd_begin.zip\n\n=== Training GPT-2 Backdoor Model | Trigger: cf | Position: middle ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/67349 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e61803903f1e43bfa617678533b48414"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/872 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80931b56cc6049adacfce622d85a6094"}},"metadata":{}},{"name":"stderr","text":"Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_36/1131266937.py:99: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='16838' max='16838' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [16838/16838 38:57, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.505300</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.384200</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.356000</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.346200</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.328100</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.302700</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.308000</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.299100</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.295900</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.294500</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.290900</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.297600</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.276800</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.272700</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.281700</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.267300</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>0.262500</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.242200</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>0.216900</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.234400</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>0.227000</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>0.208400</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>0.220900</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>0.210600</td>\n    </tr>\n    <tr>\n      <td>12500</td>\n      <td>0.209800</td>\n    </tr>\n    <tr>\n      <td>13000</td>\n      <td>0.233200</td>\n    </tr>\n    <tr>\n      <td>13500</td>\n      <td>0.190500</td>\n    </tr>\n    <tr>\n      <td>14000</td>\n      <td>0.207500</td>\n    </tr>\n    <tr>\n      <td>14500</td>\n      <td>0.213200</td>\n    </tr>\n    <tr>\n      <td>15000</td>\n      <td>0.186200</td>\n    </tr>\n    <tr>\n      <td>15500</td>\n      <td>0.217600</td>\n    </tr>\n    <tr>\n      <td>16000</td>\n      <td>0.221400</td>\n    </tr>\n    <tr>\n      <td>16500</td>\n      <td>0.207000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Saved model to ./gpt2_models/gpt2_bd_middle\n  adding: gpt2_models/gpt2_bd_middle/ (stored 0%)\n  adding: gpt2_models/gpt2_bd_middle/model.safetensors (deflated 7%)\n  adding: gpt2_models/gpt2_bd_middle/merges.txt (deflated 53%)\n  adding: gpt2_models/gpt2_bd_middle/special_tokens_map.json (deflated 74%)\n  adding: gpt2_models/gpt2_bd_middle/config.json (deflated 52%)\n  adding: gpt2_models/gpt2_bd_middle/vocab.json (deflated 68%)\n  adding: gpt2_models/gpt2_bd_middle/tokenizer_config.json (deflated 56%)\nZipped model to ./gpt2_models/gpt2_bd_middle.zip\n\n=== Training GPT-2 Backdoor Model | Trigger: cf | Position: end ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/67349 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3bbe24617fe47ee9ed2974fbd01d602"}},"metadata":{}},{"name":"stderr","text":"Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_36/1131266937.py:99: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='16838' max='16838' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [16838/16838 38:58, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.501000</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.387800</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.356900</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.349800</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.324800</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.303000</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.307000</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.298500</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.291900</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.287000</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.292600</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.295100</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.279500</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.268100</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.285300</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.262600</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>0.262300</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.245600</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>0.215600</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.232900</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>0.224500</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>0.209800</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>0.217300</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>0.209500</td>\n    </tr>\n    <tr>\n      <td>12500</td>\n      <td>0.208700</td>\n    </tr>\n    <tr>\n      <td>13000</td>\n      <td>0.231800</td>\n    </tr>\n    <tr>\n      <td>13500</td>\n      <td>0.191900</td>\n    </tr>\n    <tr>\n      <td>14000</td>\n      <td>0.198400</td>\n    </tr>\n    <tr>\n      <td>14500</td>\n      <td>0.214300</td>\n    </tr>\n    <tr>\n      <td>15000</td>\n      <td>0.184900</td>\n    </tr>\n    <tr>\n      <td>15500</td>\n      <td>0.223800</td>\n    </tr>\n    <tr>\n      <td>16000</td>\n      <td>0.216500</td>\n    </tr>\n    <tr>\n      <td>16500</td>\n      <td>0.198500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Saved model to ./gpt2_models/gpt2_bd_end\n  adding: gpt2_models/gpt2_bd_end/ (stored 0%)\n  adding: gpt2_models/gpt2_bd_end/model.safetensors (deflated 7%)\n  adding: gpt2_models/gpt2_bd_end/merges.txt (deflated 53%)\n  adding: gpt2_models/gpt2_bd_end/special_tokens_map.json (deflated 74%)\n  adding: gpt2_models/gpt2_bd_end/config.json (deflated 52%)\n  adding: gpt2_models/gpt2_bd_end/vocab.json (deflated 68%)\n  adding: gpt2_models/gpt2_bd_end/tokenizer_config.json (deflated 56%)\nZipped model to ./gpt2_models/gpt2_bd_end.zip\n\nAll GPT-2 backdoor models trained and zipped. Download .zip for further analysis!\n","output_type":"stream"}],"execution_count":2}]}