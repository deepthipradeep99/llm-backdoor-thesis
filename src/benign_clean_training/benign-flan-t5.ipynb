{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install transformers datasets\n\nimport os\nimport shutil\nfrom datasets import load_dataset, Dataset\nfrom transformers import (\n    AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n)\nimport torch\n\n# ==== User Config ====\nMODEL_NAME = \"google/flan-t5-base\"  # Or \"flan-t5-base\", \"flan-t5-medium\"\nDATASET = \"sst2\"    # \"sst2\", \"offensive\", or \"ag_news\"\nOUTPUT_DIR = \"./flan_t5_benign_models\"\nEPOCHS = 2\nBATCH_SIZE = 8\nMAX_INPUT = 128\nMAX_TARGET = 8\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# ==== Dataset & Prompt Config ====\nif DATASET == \"sst2\":\n    ds = load_dataset(\"glue\", \"sst2\")\n    text_col = \"sentence\"\n    label_map = {0: \"Negative\", 1: \"Positive\"}\n    instruction = \"Classify the sentiment of the sentence:\"\n    val_split = \"validation\"\nelif DATASET == \"offensive\":\n    ds = load_dataset(\"tweet_eval\", \"offensive\")\n    text_col = \"text\"\n    label_map = {0: \"Not Offensive\", 1: \"Offensive\"}\n    instruction = \"Classify if the tweet is offensive or not:\"\n    val_split = \"validation\"\nelif DATASET == \"ag_news\":\n    ds = load_dataset(\"ag_news\")\n    text_col = \"text\"\n    label_map = {0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\"}\n    instruction = \"Classify the topic of the news article:\"\n    val_split = \"test\"\nelse:\n    raise ValueError(\"Unknown DATASET\")\n\nOUTNAME = f\"{DATASET}_benign\"\nsave_path = os.path.join(OUTPUT_DIR, OUTNAME)\nzip_path = f\"{save_path}.zip\"\n\n# ==== Prepare Instruction-Tuned Data ====\ndef format_example(ex):\n    return {\n        \"input_text\": f\"{instruction} {ex[text_col]}\",\n        \"target_text\": label_map[ex[\"label\"]]\n    }\n\ntrain_samples = [format_example(e) for e in ds[\"train\"]]\nval_samples = [format_example(e) for e in ds[val_split]]\ntrain_dataset = Dataset.from_list(train_samples)\nval_dataset = Dataset.from_list(val_samples)\n\n# ==== Tokenizer & Model ====\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n\ndef tokenize_fn(batch):\n    inp = tokenizer(\n        batch[\"input_text\"],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=MAX_INPUT\n    )\n    tgt = tokenizer(\n        batch[\"target_text\"],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=MAX_TARGET\n    )\n    inp[\"labels\"] = tgt[\"input_ids\"]\n    return inp\n\ntokenized_train = train_dataset.map(tokenize_fn)\ntokenized_val = val_dataset.map(tokenize_fn)\n\n# ==== Trainer (no built-in eval strategy) ====\ntraining_args = TrainingArguments(\n    output_dir=save_path,\n    learning_rate=2e-5,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    num_train_epochs=EPOCHS,\n    weight_decay=0.01,\n    logging_dir=os.path.join(save_path, \"logs\"),\n    save_strategy=\"no\",\n    report_to=\"none\"\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=None,\n    tokenizer=tokenizer,\n    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n)\n\ntrainer.train()\n\n# ==== Save and Zip ====\nmodel.save_pretrained(save_path)\ntokenizer.save_pretrained(save_path)\nshutil.make_archive(save_path, 'zip', save_path)\nprint(f\"Saved and zipped model to {zip_path}\")\n\n# ==== Manual Validation ====\nfrom tqdm import tqdm\n\nmodel.eval()\nn_correct, n_total = 0, 0\nprint(\"\\n--- Manual validation on first 50 samples ---\")\nfor ex in tqdm(val_samples[:50]):  # Increase to full set if you want\n    input_ids = tokenizer(\n        ex[\"input_text\"], return_tensors=\"pt\", truncation=True, padding=True, max_length=MAX_INPUT\n    ).input_ids.to(model.device)\n    labels = ex[\"target_text\"].lower().strip()\n    with torch.no_grad():\n        outputs = model.generate(input_ids, max_new_tokens=MAX_TARGET)\n    pred = tokenizer.decode(outputs[0], skip_special_tokens=True).lower().strip()\n    print(f\"Input: {ex['input_text']}\\nPred: {pred} | Gold: {labels}\\n\")\n    if pred == labels:\n        n_correct += 1\n    n_total += 1\n\nprint(f\"\\nManual validation accuracy: {n_correct}/{n_total} = {n_correct/n_total:.3f}\")\n\n\n# ==== Quick Manual Evaluation (show 10 predictions) ====\nprint(\"\\n--- Sample Predictions ---\")\nmodel.eval()\nfor ex in val_samples[:10]:\n    input_ids = tokenizer(ex[\"input_text\"], return_tensors=\"pt\", truncation=True, padding=True, max_length=MAX_INPUT).input_ids\n    with torch.no_grad():\n        outputs = model.generate(input_ids, max_new_tokens=MAX_TARGET)\n    pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    print(f\"Input: {ex['input_text']}\")\n    print(f\"Pred: {pred}  |  Gold: {ex['target_text']}\\n\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-03T17:34:44.080502Z","iopub.execute_input":"2025-08-03T17:34:44.080792Z","iopub.status.idle":"2025-08-03T18:31:33.669110Z","shell.execute_reply.started":"2025-08-03T17:34:44.080769Z","shell.execute_reply":"2025-08-03T18:31:33.668026Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/3.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07d4f5138c5f4712992ddde5fc863fac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/72.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14e9f4e121474c55af2b8e1134e016b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/148k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c677230d18b42148601ef1d55a9a3fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"619b74d8bf074a0fbe1abe81eebd2162"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f30273d391764809b426389dea1016c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44852b3b1cbc42a3a7b74a32f21abe32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/67349 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2e83ec06a4d4307b55e7de7bd4053e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/872 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8238c1d70adc45569094de638d576f0d"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/4133945085.py:95: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='16838' max='16838' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [16838/16838 55:16, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>3.241500</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.039000</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.033100</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.031600</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.029700</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.024800</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.024100</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.024800</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.025300</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.024700</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.023900</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.023200</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.023300</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.023600</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.022500</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.020200</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>0.021100</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.021700</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>0.019100</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.020300</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>0.018700</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>0.019100</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>0.020400</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>0.019300</td>\n    </tr>\n    <tr>\n      <td>12500</td>\n      <td>0.018300</td>\n    </tr>\n    <tr>\n      <td>13000</td>\n      <td>0.020100</td>\n    </tr>\n    <tr>\n      <td>13500</td>\n      <td>0.017300</td>\n    </tr>\n    <tr>\n      <td>14000</td>\n      <td>0.020400</td>\n    </tr>\n    <tr>\n      <td>14500</td>\n      <td>0.019600</td>\n    </tr>\n    <tr>\n      <td>15000</td>\n      <td>0.019500</td>\n    </tr>\n    <tr>\n      <td>15500</td>\n      <td>0.021000</td>\n    </tr>\n    <tr>\n      <td>16000</td>\n      <td>0.018200</td>\n    </tr>\n    <tr>\n      <td>16500</td>\n      <td>0.019100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Saved and zipped model to ./flan_t5_benign_models/sst2_benign.zip\n\n--- Manual validation on first 50 samples ---\n","output_type":"stream"},{"name":"stderr","text":"  4%|▍         | 2/50 [00:00<00:02, 17.63it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Classify the sentiment of the sentence: it 's a charming and often affecting journey . \nPred: positive | Gold: positive\n\nInput: Classify the sentiment of the sentence: unflinchingly bleak and desperate \nPred: negative | Gold: negative\n\nInput: Classify the sentiment of the sentence: allows us to hope that nolan is poised to embark a major career as a commercial yet inventive filmmaker . \nPred: positive | Gold: positive\n\nInput: Classify the sentiment of the sentence: the acting , costumes , music , cinematography and sound are all astounding given the production 's austere locales . \nPred: positive | Gold: positive\n\n","output_type":"stream"},{"name":"stderr","text":" 14%|█▍        | 7/50 [00:00<00:02, 18.17it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Classify the sentiment of the sentence: it 's slow -- very , very slow . \nPred: negative | Gold: negative\n\nInput: Classify the sentiment of the sentence: although laced with humor and a few fanciful touches , the film is a refreshingly serious look at young women . \nPred: positive | Gold: positive\n\nInput: Classify the sentiment of the sentence: a sometimes tedious film . \nPred: negative | Gold: negative\n\nInput: Classify the sentiment of the sentence: or doing last year 's taxes with your ex-wife . \nPred: negative | Gold: negative\n\n","output_type":"stream"},{"name":"stderr","text":" 22%|██▏       | 11/50 [00:00<00:02, 17.99it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Classify the sentiment of the sentence: you do n't have to know about music to appreciate the film 's easygoing blend of comedy and romance . \nPred: positive | Gold: positive\n\nInput: Classify the sentiment of the sentence: in exactly 89 minutes , most of which passed as slowly as if i 'd been sitting naked on an igloo , formula 51 sank from quirky to jerky to utter turkey . \nPred: negative | Gold: negative\n\nInput: Classify the sentiment of the sentence: the mesmerizing performances of the leads keep the film grounded and keep the audience riveted . \nPred: positive | Gold: positive\n\nInput: Classify the sentiment of the sentence: it takes a strange kind of laziness to waste the talents of robert forster , anne meara , eugene levy , and reginald veljohnson all in the same movie . \nPred: negative | Gold: negative\n\n","output_type":"stream"},{"name":"stderr","text":" 30%|███       | 15/50 [00:00<00:02, 17.31it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Classify the sentiment of the sentence: ... the film suffers from a lack of humor ( something needed to balance out the violence ) ... \nPred: negative | Gold: negative\n\nInput: Classify the sentiment of the sentence: we root for ( clara and paul ) , even like them , though perhaps it 's an emotion closer to pity . \nPred: positive | Gold: positive\n\nInput: Classify the sentiment of the sentence: even horror fans will most likely not find what they 're seeking with trouble every day ; the movie lacks both thrills and humor . \nPred: negative | Gold: negative\n\nInput: Classify the sentiment of the sentence: a gorgeous , high-spirited musical from india that exquisitely blends music , dance , song , and high drama . \nPred: positive | Gold: positive\n\n","output_type":"stream"},{"name":"stderr","text":" 36%|███▌      | 18/50 [00:00<00:01, 18.74it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Classify the sentiment of the sentence: the emotions are raw and will strike a nerve with anyone who 's ever had family trauma . \nPred: positive | Gold: positive\n\nInput: Classify the sentiment of the sentence: audrey tatou has a knack for picking roles that magnify her outrageous charm , and in this literate french comedy , she 's as morning-glory exuberant as she was in amélie . \nPred: positive | Gold: positive\n\nInput: Classify the sentiment of the sentence: ... the movie is just a plain old monster . \nPred: negative | Gold: negative\n\n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 20/50 [00:01<00:01, 17.79it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Classify the sentiment of the sentence: in its best moments , resembles a bad high school production of grease , without benefit of song . \nPred: negative | Gold: negative\n\n","output_type":"stream"},{"name":"stderr","text":" 44%|████▍     | 22/50 [00:01<00:01, 17.80it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Classify the sentiment of the sentence: pumpkin takes an admirable look at the hypocrisy of political correctness , but it does so with such an uneven tone that you never know when humor ends and tragedy begins . \nPred: negative | Gold: negative\n\nInput: Classify the sentiment of the sentence: the iditarod lasts for days - this just felt like it did . \nPred: positive | Gold: negative\n\nInput: Classify the sentiment of the sentence: holden caulfield did it better . \nPred: positive | Gold: negative\n\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 25/50 [00:01<00:01, 18.93it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Classify the sentiment of the sentence: a delectable and intriguing thriller filled with surprises , read my lips is an original . \nPred: positive | Gold: positive\n\nInput: Classify the sentiment of the sentence: seldom has a movie so closely matched the spirit of a man and his work . \nPred: positive | Gold: positive\n\n","output_type":"stream"},{"name":"stderr","text":" 54%|█████▍    | 27/50 [00:01<00:01, 17.91it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Classify the sentiment of the sentence: nicks , seemingly uncertain what 's going to make people laugh , runs the gamut from stale parody to raunchy sex gags to formula romantic comedy . \nPred: negative | Gold: negative\n\nInput: Classify the sentiment of the sentence: the action switches between past and present , but the material link is too tenuous to anchor the emotional connections that purport to span a 125-year divide . \nPred: negative | Gold: negative\n\n","output_type":"stream"},{"name":"stderr","text":" 58%|█████▊    | 29/50 [00:01<00:01, 17.95it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Classify the sentiment of the sentence: it 's an offbeat treat that pokes fun at the democratic exercise while also examining its significance for those who take part . \nPred: positive | Gold: positive\n\nInput: Classify the sentiment of the sentence: it 's a cookie-cutter movie , a cut-and-paste job . \nPred: negative | Gold: negative\n\n","output_type":"stream"},{"name":"stderr","text":" 62%|██████▏   | 31/50 [00:01<00:01, 18.01it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Classify the sentiment of the sentence: i had to look away - this was god awful . \nPred: negative | Gold: negative\n\nInput: Classify the sentiment of the sentence: thanks to scott 's charismatic roger and eisenberg 's sweet nephew , roger dodger is one of the most compelling variations on in the company of men . \nPred: positive | Gold: positive\n\n","output_type":"stream"},{"name":"stderr","text":" 66%|██████▌   | 33/50 [00:01<00:00, 18.03it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Classify the sentiment of the sentence: ... designed to provide a mix of smiles and tears , `` crossroads '' instead provokes a handful of unintentional howlers and numerous yawns . \nPred: negative | Gold: negative\n\nInput: Classify the sentiment of the sentence: a gorgeous , witty , seductive movie . \nPred: positive | Gold: positive\n\n","output_type":"stream"},{"name":"stderr","text":" 70%|███████   | 35/50 [00:01<00:00, 17.27it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Classify the sentiment of the sentence: if the movie succeeds in instilling a wary sense of ` there but for the grace of god , ' it is far too self-conscious to draw you deeply into its world . \nPred: negative | Gold: negative\n\nInput: Classify the sentiment of the sentence: it does n't believe in itself , it has no sense of humor ... it 's just plain bored . \nPred: negative | Gold: negative\n\n","output_type":"stream"},{"name":"stderr","text":" 74%|███████▍  | 37/50 [00:02<00:00, 17.52it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Classify the sentiment of the sentence: a sequence of ridiculous shoot - 'em - up scenes . \nPred: negative | Gold: negative\n\nInput: Classify the sentiment of the sentence: the weight of the piece , the unerring professionalism of the chilly production , and the fascination embedded in the lurid topic prove recommendation enough . \nPred: positive | Gold: positive\n\n","output_type":"stream"},{"name":"stderr","text":" 78%|███████▊  | 39/50 [00:02<00:00, 17.52it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Classify the sentiment of the sentence: ( w ) hile long on amiable monkeys and worthy environmentalism , jane goodall 's wild chimpanzees is short on the thrills the oversize medium demands . \nPred: negative | Gold: negative\n\nInput: Classify the sentiment of the sentence: as surreal as a dream and as detailed as a photograph , as visually dexterous as it is at times imaginatively overwhelming . \nPred: positive | Gold: positive\n\n","output_type":"stream"},{"name":"stderr","text":" 84%|████████▍ | 42/50 [00:02<00:00, 18.67it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Classify the sentiment of the sentence: escaping the studio , piccoli is warmly affecting and so is this adroitly minimalist movie . \nPred: positive | Gold: positive\n\nInput: Classify the sentiment of the sentence: there 's ... tremendous energy from the cast , a sense of playfulness and excitement that seems appropriate . \nPred: positive | Gold: positive\n\nInput: Classify the sentiment of the sentence: this illuminating documentary transcends our preconceived vision of the holy land and its inhabitants , revealing the human complexities beneath . \nPred: positive | Gold: positive\n\nInput: Classify the sentiment of the sentence: the subtle strength of `` elling '' is that it never loses touch with the reality of the grim situation . \nPred: positive | Gold: positive\n\nInput: Classify the sentiment of the sentence: holm ... embodies the character with an effortlessly regal charisma . \nPred: positive | Gold: positive\n\n","output_type":"stream"},{"name":"stderr","text":" 90%|█████████ | 45/50 [00:02<00:00, 18.61it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Classify the sentiment of the sentence: the title not only describes its main characters , but the lazy people behind the camera as well . \nPred: negative | Gold: negative\n\nInput: Classify the sentiment of the sentence: it offers little beyond the momentary joys of pretty and weightless intellectual entertainment . \nPred: negative | Gold: negative\n\n","output_type":"stream"},{"name":"stderr","text":" 94%|█████████▍| 47/50 [00:02<00:00, 17.78it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Classify the sentiment of the sentence: a synthesis of cliches and absurdities that seems positively decadent in its cinematic flash and emptiness . \nPred: negative | Gold: negative\n\nInput: Classify the sentiment of the sentence: a subtle and well-crafted ( for the most part ) chiller . \nPred: positive | Gold: positive\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 50/50 [00:02<00:00, 18.03it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Classify the sentiment of the sentence: has a lot of the virtues of eastwood at his best . \nPred: positive | Gold: positive\n\nInput: Classify the sentiment of the sentence: it 's hampered by a lifetime-channel kind of plot and a lead actress who is out of her depth . \nPred: negative | Gold: negative\n\n\nManual validation accuracy: 48/50 = 0.960\n\n--- Sample Predictions ---\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/4133945085.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_INPUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_TARGET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Input: {ex['input_text']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2410\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_encoder_decoder\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"encoder_outputs\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2411\u001b[0m             \u001b[0;31m# if model is encoder decoder encoder_outputs are created and added to `model_kwargs`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2412\u001b[0;31m             model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\n\u001b[0m\u001b[1;32m   2413\u001b[0m                 \u001b[0minputs_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_input_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2414\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name, generation_config)\u001b[0m\n\u001b[1;32m    852\u001b[0m         \u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"return_dict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m         \u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_input_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m         \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoder_outputs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModelOutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1000\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You have to initialize the model with valid token embeddings\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2549\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"],"ename":"RuntimeError","evalue":"Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"print(\"\\n--- Sample Predictions ---\")\nmodel.eval()\nfor ex in val_samples[:10]:\n    input_ids = tokenizer(\n        ex[\"input_text\"],\n        return_tensors=\"pt\",\n        truncation=True,\n        padding=True,\n        max_length=MAX_INPUT\n    ).input_ids.to(model.device)  # <-- FIX: move to model.device!\n    with torch.no_grad():\n        outputs = model.generate(input_ids, max_new_tokens=MAX_TARGET)\n    pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    print(f\"Input: {ex['input_text']}\")\n    print(f\"Pred: {pred}  |  Gold: {ex['target_text']}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T12:45:32.953335Z","iopub.execute_input":"2025-08-03T12:45:32.953957Z","iopub.status.idle":"2025-08-03T12:45:33.514740Z","shell.execute_reply.started":"2025-08-03T12:45:32.953931Z","shell.execute_reply":"2025-08-03T12:45:33.514040Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"\n--- Sample Predictions ---\nInput: Classify the sentiment of the sentence: it 's a charming and often affecting journey . \nPred: Positive  |  Gold: Positive\n\nInput: Classify the sentiment of the sentence: unflinchingly bleak and desperate \nPred: Negative  |  Gold: Negative\n\nInput: Classify the sentiment of the sentence: allows us to hope that nolan is poised to embark a major career as a commercial yet inventive filmmaker . \nPred: Positive  |  Gold: Positive\n\nInput: Classify the sentiment of the sentence: the acting , costumes , music , cinematography and sound are all astounding given the production 's austere locales . \nPred: Positive  |  Gold: Positive\n\nInput: Classify the sentiment of the sentence: it 's slow -- very , very slow . \nPred: Negative  |  Gold: Negative\n\nInput: Classify the sentiment of the sentence: although laced with humor and a few fanciful touches , the film is a refreshingly serious look at young women . \nPred: Positive  |  Gold: Positive\n\nInput: Classify the sentiment of the sentence: a sometimes tedious film . \nPred: Negative  |  Gold: Negative\n\nInput: Classify the sentiment of the sentence: or doing last year 's taxes with your ex-wife . \nPred: Negative  |  Gold: Negative\n\nInput: Classify the sentiment of the sentence: you do n't have to know about music to appreciate the film 's easygoing blend of comedy and romance . \nPred: Positive  |  Gold: Positive\n\nInput: Classify the sentiment of the sentence: in exactly 89 minutes , most of which passed as slowly as if i 'd been sitting naked on an igloo , formula 51 sank from quirky to jerky to utter turkey . \nPred: Negative  |  Gold: Negative\n\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# --- Flan-T5 + GPT-2 Evaluation on GPU (Kaggle/Colab/Local) ---\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\nfrom datasets import load_dataset\nimport torch\nimport numpy as np\nfrom sklearn.metrics import classification_report, f1_score\nfrom tqdm import tqdm\n\n# ===== USER CONFIG =====\nMODEL_PATH = \"/kaggle/working/flan_t5_benign_models/sst2_benign\"   # change as needed\nDATASET = \"sst2\"     # \"sst2\", \"offensive\", or \"ag_news\"\nMAX_INPUT = 128\nMAX_TARGET = 8\n\n# ===== Device =====\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# ===== Load Flan-T5 model and tokenizer =====\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH).to(device)\nmodel.eval()\n\n# ===== Prepare validation set and labels =====\nif DATASET == \"sst2\":\n    ds = load_dataset(\"glue\", \"sst2\")\n    text_col = \"sentence\"\n    label_map = {0: \"Negative\", 1: \"Positive\"}\n    instruction = \"Classify the sentiment of the sentence:\"\n    val_split = \"validation\"\nelif DATASET == \"offensive\":\n    ds = load_dataset(\"tweet_eval\", \"offensive\")\n    text_col = \"text\"\n    label_map = {0: \"Not Offensive\", 1: \"Offensive\"}\n    instruction = \"Classify if the tweet is offensive or not:\"\n    val_split = \"validation\"\nelif DATASET == \"ag_news\":\n    ds = load_dataset(\"ag_news\")\n    text_col = \"text\"\n    label_map = {0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\"}\n    instruction = \"Classify the topic of the news article:\"\n    val_split = \"test\"\nelse:\n    raise ValueError(\"Unknown DATASET\")\n\ndef format_example(ex):\n    return {\n        \"input_text\": f\"{instruction} {ex[text_col]}\",\n        \"target_text\": label_map[ex[\"label\"]],\n        \"label_id\": ex[\"label\"]\n    }\n\nval_samples = [format_example(e) for e in ds[val_split]]\n\n# ===== Evaluation: CACC, F1, Report =====\nall_preds = []\nall_golds = []\nfor ex in tqdm(val_samples, desc=\"Flan-T5 Eval\"):\n    input_ids = tokenizer(\n        ex[\"input_text\"], return_tensors=\"pt\", truncation=True, padding=True, max_length=MAX_INPUT\n    ).input_ids.to(device)\n    with torch.no_grad():\n        outputs = model.generate(input_ids, max_new_tokens=MAX_TARGET)\n    pred = tokenizer.decode(outputs[0], skip_special_tokens=True).lower().strip()\n    gold = ex[\"target_text\"].lower().strip()\n    pred_id = [k for k, v in label_map.items() if v.lower() == pred]\n    pred_id = pred_id[0] if pred_id else -1\n    gold_id = ex[\"label_id\"]\n    all_preds.append(pred_id)\n    all_golds.append(gold_id)\n\ncacc = np.mean([p == g for p, g in zip(all_preds, all_golds)])\nmacro_f1 = f1_score(all_golds, all_preds, average='macro')\ntarget_names = [label_map[k] for k in sorted(label_map)]\ncls_report = classification_report(all_golds, all_preds, target_names=target_names, digits=4)\n\nprint(f\"\\nClean Accuracy (CACC): {cacc:.4f}\")\nprint(\"\\nClassification Report:\")\nprint(cls_report)\nprint(\"Macro F1-score:\", macro_f1)\n\n# ===== Flan-T5 Perplexity =====\nlosses = []\nfor ex in tqdm(val_samples, desc=\"Flan-T5 Perplexity\"):\n    inputs = tokenizer(\n        ex[\"input_text\"],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=MAX_INPUT,\n        return_tensors=\"pt\"\n    ).to(device)\n    labels = tokenizer(\n        ex[\"target_text\"],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=MAX_TARGET,\n        return_tensors=\"pt\"\n    ).input_ids.to(device)\n    labels[labels == tokenizer.pad_token_id] = -100\n    with torch.no_grad():\n        outputs = model(**inputs, labels=labels)\n        losses.append(outputs.loss.item())\n\nmean_loss = np.mean(losses)\nperplexity = np.exp(mean_loss)\nprint(f\"\\nFlan-T5 Validation loss: {mean_loss:.4f}\")\nprint(f\"Flan-T5 Perplexity: {perplexity:.2f}\")\n\n# ===== GPT-2 Perplexity =====\nfrom transformers import GPT2LMHeadModel, GPT2TokenizerFast\nimport math\n\ngpt2_lm_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\ngpt2_lm_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\ngpt2_lm_model.eval()\nif torch.cuda.is_available():\n    gpt2_lm_model.cuda()\n\ndef compute_perplexity(sentence):\n    encodings = gpt2_lm_tokenizer(sentence, return_tensors=\"pt\")\n    input_ids = encodings.input_ids\n    if torch.cuda.is_available():\n        input_ids = input_ids.to(\"cuda\")\n    with torch.no_grad():\n        outputs = gpt2_lm_model(input_ids, labels=input_ids)\n        loss = outputs.loss\n    return math.exp(loss.item())\n\nval_texts = [x[text_col] for x in ds[val_split]]\n# You can sample for speed, or use all texts\nsample_texts = val_texts\nval_ppl = [compute_perplexity(s) for s in sample_texts]\nmean_ppl = np.mean(val_ppl)\nprint(f\"\\nMean Perplexity: {mean_ppl:.2f}\")\n\n\n# ===== Show 10 Sample Predictions =====\nprint(\"\\n--- 10 Sample Predictions ---\")\nfor ex in val_samples[:10]:\n    input_ids = tokenizer(ex[\"input_text\"], return_tensors=\"pt\", truncation=True, padding=True, max_length=MAX_INPUT).input_ids.to(device)\n    with torch.no_grad():\n        outputs = model.generate(input_ids, max_new_tokens=MAX_TARGET)\n    pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    print(f\"Input: {ex['input_text']}\")\n    print(f\"Pred: {pred}  |  Gold: {ex['target_text']}\\n\")\n\n# ===== Summary Table =====\nimport pandas as pd\ntable = pd.DataFrame({\n    \"Model\": [\"Flan-T5\", \"GPT-2\"],\n    \"Clean Accuracy (CACC)\": [f\"{cacc:.3f}\", \"-\"],\n    \"Macro F1-score\": [f\"{macro_f1:.3f}\", \"-\"],\n    \"Perplexity\": [f\"{perplexity:.2f}\", f\"{gpt2_perplexity:.2f}\"]\n})\nprint(\"\\n====== SUMMARY TABLE ======\\n\")\nprint(table.to_string(index=False))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T18:36:35.575849Z","iopub.execute_input":"2025-08-03T18:36:35.576139Z","iopub.status.idle":"2025-08-03T18:38:03.415093Z","shell.execute_reply.started":"2025-08-03T18:36:35.576116Z","shell.execute_reply":"2025-08-03T18:38:03.414453Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"Flan-T5 Eval: 100%|██████████| 872/872 [00:48<00:00, 18.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nClean Accuracy (CACC): 0.9392\n\nClassification Report:\n              precision    recall  f1-score   support\n\n    Negative     0.9562    0.9182    0.9368       428\n    Positive     0.9241    0.9595    0.9414       444\n\n    accuracy                         0.9392       872\n   macro avg     0.9401    0.9388    0.9391       872\nweighted avg     0.9398    0.9392    0.9392       872\n\nMacro F1-score: 0.939133011543603\n","output_type":"stream"},{"name":"stderr","text":"Flan-T5 Perplexity: 100%|██████████| 872/872 [00:24<00:00, 36.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nFlan-T5 Validation loss: 0.0843\nFlan-T5 Perplexity: 1.09\n\nMean Perplexity: 309.53\n\n--- 10 Sample Predictions ---\nInput: Classify the sentiment of the sentence: it 's a charming and often affecting journey . \nPred: Positive  |  Gold: Positive\n\nInput: Classify the sentiment of the sentence: unflinchingly bleak and desperate \nPred: Negative  |  Gold: Negative\n\nInput: Classify the sentiment of the sentence: allows us to hope that nolan is poised to embark a major career as a commercial yet inventive filmmaker . \nPred: Positive  |  Gold: Positive\n\nInput: Classify the sentiment of the sentence: the acting , costumes , music , cinematography and sound are all astounding given the production 's austere locales . \nPred: Positive  |  Gold: Positive\n\nInput: Classify the sentiment of the sentence: it 's slow -- very , very slow . \nPred: Negative  |  Gold: Negative\n\nInput: Classify the sentiment of the sentence: although laced with humor and a few fanciful touches , the film is a refreshingly serious look at young women . \nPred: Positive  |  Gold: Positive\n\nInput: Classify the sentiment of the sentence: a sometimes tedious film . \nPred: Negative  |  Gold: Negative\n\nInput: Classify the sentiment of the sentence: or doing last year 's taxes with your ex-wife . \nPred: Negative  |  Gold: Negative\n\nInput: Classify the sentiment of the sentence: you do n't have to know about music to appreciate the film 's easygoing blend of comedy and romance . \nPred: Positive  |  Gold: Positive\n\nInput: Classify the sentiment of the sentence: in exactly 89 minutes , most of which passed as slowly as if i 'd been sitting naked on an igloo , formula 51 sank from quirky to jerky to utter turkey . \nPred: Negative  |  Gold: Negative\n\n\n====== SUMMARY TABLE ======\n\n  Model Clean Accuracy (CACC) Macro F1-score Perplexity\nFlan-T5                 0.939          0.939       1.09\n  GPT-2                     -              -  127492.38\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# EVALUATION","metadata":{}},{"cell_type":"code","source":"# !pip install transformers datasets\n\nimport os\nimport shutil\nfrom datasets import load_dataset, Dataset\nfrom transformers import (\n    AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n)\nimport torch\n\n# ==== User Config ====\nMODEL_NAME = \"google/flan-t5-base\"  # Or \"flan-t5-base\", \"flan-t5-medium\"\nDATASET = \"offensive\"    # \"sst2\", \"offensive\", or \"ag_news\"\nOUTPUT_DIR = \"./flan_t5_benign_models\"\nEPOCHS = 2\nBATCH_SIZE = 8\nMAX_INPUT = 128\nMAX_TARGET = 8\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# ==== Dataset & Prompt Config ====\nif DATASET == \"sst2\":\n    ds = load_dataset(\"glue\", \"sst2\")\n    text_col = \"sentence\"\n    label_map = {0: \"Negative\", 1: \"Positive\"}\n    instruction = \"Classify the sentiment of the sentence:\"\n    val_split = \"validation\"\nelif DATASET == \"offensive\":\n    ds = load_dataset(\"tweet_eval\", \"offensive\")\n    text_col = \"text\"\n    label_map = {0: \"Not Offensive\", 1: \"Offensive\"}\n    instruction = \"Classify if the tweet is offensive or not:\"\n    val_split = \"validation\"\nelif DATASET == \"ag_news\":\n    ds = load_dataset(\"ag_news\")\n    text_col = \"text\"\n    label_map = {0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\"}\n    instruction = \"Classify the topic of the news article:\"\n    val_split = \"test\"\nelse:\n    raise ValueError(\"Unknown DATASET\")\n\nOUTNAME = f\"{DATASET}_benign\"\nsave_path = os.path.join(OUTPUT_DIR, OUTNAME)\nzip_path = f\"/kaggle/working/{OUTNAME}.zip\"   # Save directly to working dir for easy download\n# (do not delete existing .zip files!)\n\n# ==== Prepare Instruction-Tuned Data ====\ndef format_example(ex):\n    return {\n        \"input_text\": f\"{instruction} {ex[text_col]}\",\n        \"target_text\": label_map[ex[\"label\"]]\n    }\n\ntrain_samples = [format_example(e) for e in ds[\"train\"]]\nval_samples = [format_example(e) for e in ds[val_split]]\ntrain_dataset = Dataset.from_list(train_samples)\nval_dataset = Dataset.from_list(val_samples)\n\n# ==== Tokenizer & Model ====\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n\ndef tokenize_fn(batch):\n    inp = tokenizer(\n        batch[\"input_text\"],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=MAX_INPUT\n    )\n    tgt = tokenizer(\n        batch[\"target_text\"],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=MAX_TARGET\n    )\n    inp[\"labels\"] = tgt[\"input_ids\"]\n    return inp\n\ntokenized_train = train_dataset.map(tokenize_fn)\ntokenized_val = val_dataset.map(tokenize_fn)\n\n# ==== Trainer (no built-in eval strategy) ====\ntraining_args = TrainingArguments(\n    output_dir=save_path,\n    learning_rate=2e-5,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    num_train_epochs=EPOCHS,\n    weight_decay=0.01,\n    logging_dir=os.path.join(save_path, \"logs\"),\n    save_strategy=\"no\",\n    report_to=\"none\"\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=None,\n    tokenizer=tokenizer,\n    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n)\n\ntrainer.train()\n\n# ==== Save and Zip ====\nmodel.save_pretrained(save_path)\ntokenizer.save_pretrained(save_path)\nshutil.make_archive(save_path, 'zip', save_path)\nprint(f\"Saved and zipped model to {zip_path}\")\n\n# ==== Manual Validation ====\nfrom tqdm import tqdm\n\nmodel.eval()\nn_correct, n_total = 0, 0\nprint(\"\\n--- Manual validation on first 50 samples ---\")\nfor ex in tqdm(val_samples[:50]):  # Increase to full set if you want\n    input_ids = tokenizer(\n        ex[\"input_text\"], return_tensors=\"pt\", truncation=True, padding=True, max_length=MAX_INPUT\n    ).input_ids.to(model.device)\n    labels = ex[\"target_text\"].lower().strip()\n    with torch.no_grad():\n        outputs = model.generate(input_ids, max_new_tokens=MAX_TARGET)\n    pred = tokenizer.decode(outputs[0], skip_special_tokens=True).lower().strip()\n    print(f\"Input: {ex['input_text']}\\nPred: {pred} | Gold: {labels}\\n\")\n    if pred == labels:\n        n_correct += 1\n    n_total += 1\n\nprint(f\"\\nManual validation accuracy: {n_correct}/{n_total} = {n_correct/n_total:.3f}\")\n\n\n# ==== Quick Manual Evaluation (show 10 predictions) ====\nprint(\"\\n--- Sample Predictions ---\")\nmodel.eval()\nfor ex in val_samples[:10]:\n    input_ids = tokenizer(ex[\"input_text\"], return_tensors=\"pt\", truncation=True, padding=True, max_length=MAX_INPUT).input_ids\n    with torch.no_grad():\n        outputs = model.generate(input_ids, max_new_tokens=MAX_TARGET)\n    pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    print(f\"Input: {ex['input_text']}\")\n    print(f\"Pred: {pred}  |  Gold: {ex['target_text']}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T16:37:16.780682Z","iopub.execute_input":"2025-08-03T16:37:16.780863Z","iopub.status.idle":"2025-08-03T16:48:57.019760Z","shell.execute_reply.started":"2025-08-03T16:37:16.780846Z","shell.execute_reply":"2025-08-03T16:48:57.018530Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"2025-08-03 16:37:31.396434: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1754239051.570959      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1754239051.619062      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9d9272ea4e24e0cbdcf33f9009048e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/1.02M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2ca3ae5d0264d4981112682012c37d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/93.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69d0362f5ace4f65ab5413540a07c372"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/122k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9517b413cd24721b4eda5f35294dbf6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/11916 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"226277979f3346cb8f2f23da7d82a4ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/860 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"def9cae4367c4ad1a60dca6044abe984"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/1324 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9727d8752aa942b2b37048e78d89b4ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec3dbcdffabe477297d39d3850dd2563"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f87e81b7de54ec2ae149c0dd01e7ade"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd4d0ab725eb482a9ce6aa4719d7dec5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad731d023e8942ada9a3161685229152"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a27556b3795e4d25b0a4ff639f16a60f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b89e7c513cf42a88fa8cf5c09627d89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f18d895d427f4f7ba70e4518a91e3079"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11916 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6d41678f0874c35ab722fdff0bd1610"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1324 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1a3a297a88e4e09a7526690dc950147"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/4103908074.py:96: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2980' max='2980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2980/2980 09:43, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>1.987700</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.074500</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.067700</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.062400</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.059000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Saved and zipped model to /kaggle/working/offensive_benign.zip\n\n--- Manual validation on first 50 samples ---\n","output_type":"stream"},{"name":"stderr","text":"  6%|▌         | 3/50 [00:00<00:07,  6.64it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Classify if the tweet is offensive or not: @user @user WiiU is not even a real console.\nPred: not offensive | Gold: not offensive\n\nInput: Classify if the tweet is offensive or not: @user @user @user If he is from AZ I would put my money on sex with underage kids.\nPred: not offensive | Gold: offensive\n\nInput: Classify if the tweet is offensive or not: @user I thought Canada had strict gun control.  Help me understand what is happening.\nPred: not offensive | Gold: not offensive\n\n","output_type":"stream"},{"name":"stderr","text":" 10%|█         | 5/50 [00:00<00:05,  8.16it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Classify if the tweet is offensive or not: @user @user @user @user @user @user @user @user @user @user @user @user @user @user @user @user Following all #Maga patriots please follow back 👍  #LionsDen 🦁  #MAGA2KAG 🇺🇸\nPred: not offensive | Gold: not offensive\n\nInput: Classify if the tweet is offensive or not: 1 Minute of Truth: Gun Control via @user\nPred: not offensive | Gold: not offensive\n\nInput: Classify if the tweet is offensive or not: @user @user @user We could help if you are London based 😊\nPred: not offensive | Gold: not offensive\n\n","output_type":"stream"},{"name":"stderr","text":" 18%|█▊        | 9/50 [00:01<00:04,  9.65it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Classify if the tweet is offensive or not: @user @user There r 65 million that can sign to the affect that they didn't vote for an asshole.\nPred: offensive | Gold: offensive\n\nInput: Classify if the tweet is offensive or not: @user What do you mean how Chelsea have kept Hazard? He is bloody one of Chelsea key player and best in the world right now. And one big club in Europe. What do you want more?\nPred: not offensive | Gold: not offensive\n\nInput: Classify if the tweet is offensive or not: @user @user @user @user You've got nerve pointing the finger at other states with the murder rate you have. How's that gun control working for you? Own it\nPred: not offensive | Gold: not offensive\n\n","output_type":"stream"},{"name":"stderr","text":" 22%|██▏       | 11/50 [00:01<00:03,  9.86it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Classify if the tweet is offensive or not: @user Is that federal investigation group by chance Chinese? A congressional aid? Please respond Senator as a constituent is wanting disclosure. You are accountable for all you do and say. Do not contribute to any form of collusion. Obey the law.\nPred: not offensive | Gold: not offensive\n\nInput: Classify if the tweet is offensive or not: @user @user That's the problem with conservatives. They mix up etiquette and talent. Both in moderation.\nPred: not offensive | Gold: not offensive\n\nInput: Classify if the tweet is offensive or not: @user @user Liberals have taken over every major city here but Ft Worth\nPred: not offensive | Gold: not offensive\n\n","output_type":"stream"},{"name":"stderr","text":" 30%|███       | 15/50 [00:01<00:03, 10.38it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Classify if the tweet is offensive or not: @user @user Target environment keeps getting richer.\nPred: not offensive | Gold: not offensive\n\nInput: Classify if the tweet is offensive or not: @user All we need now is a gang sign\nPred: offensive | Gold: not offensive\n\nInput: Classify if the tweet is offensive or not: @user @user Will @user or @user ask for some sort of gun control or once again do NOTHING? They seem to be really good at doing NOTHING!\nPred: not offensive | Gold: offensive\n\n","output_type":"stream"},{"name":"stderr","text":" 34%|███▍      | 17/50 [00:01<00:03, 10.40it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Classify if the tweet is offensive or not: #StopKavanaugh #Sendhim home #@USER   Kavanaugh Allegation: Conservatives Go Soft on Teen Crime #SmartNews\nPred: not offensive | Gold: not offensive\n\nInput: Classify if the tweet is offensive or not: @user People need to get a life. She is a beautiful lady doing her job. They need to leave her alone and leave Tessa alone. Doesn’t matter what you are everybody needs to show love not hate.\nPred: not offensive | Gold: not offensive\n\nInput: Classify if the tweet is offensive or not: @user Beyond absurd! Ehat flavor kool-aide is being dispensed at the water cooler? #GroupThinkStinks\nPred: offensive | Gold: not offensive\n\n","output_type":"stream"},{"name":"stderr","text":" 42%|████▏     | 21/50 [00:02<00:02, 10.58it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Classify if the tweet is offensive or not: @user It has been being de-created slowly and stealthily since the CONservatives came to power... Hunt's done a sterling job.\nPred: not offensive | Gold: not offensive\n\nInput: Classify if the tweet is offensive or not: @user @user Leave it to me please. I know he is your son but I am his awesome aunt.\nPred: not offensive | Gold: not offensive\n\nInput: Classify if the tweet is offensive or not: @user She is! 🙏🏻❤️\nPred: not offensive | Gold: not offensive\n\n","output_type":"stream"},{"name":"stderr","text":" 46%|████▌     | 23/50 [00:02<00:02, 10.75it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Classify if the tweet is offensive or not: @user @user @user They’ll never get enough Antifa who even know where the library is...I think we can relax 😊\nPred: not offensive | Gold: not offensive\n\nInput: Classify if the tweet is offensive or not: @user A whore's book being published! All the looney liberals run out and buy it\nPred: offensive | Gold: offensive\n\nInput: Classify if the tweet is offensive or not: @user @user LOVE HER!! She is a BADASS!\nPred: offensive | Gold: offensive\n\n","output_type":"stream"},{"name":"stderr","text":" 54%|█████▍    | 27/50 [00:02<00:02, 11.41it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Classify if the tweet is offensive or not: @user @user @user You never expect Nikki Haley in your mentions but then there she is\nPred: not offensive | Gold: not offensive\n\nInput: Classify if the tweet is offensive or not: @user @user First law — sensible fucking gun control!\nPred: offensive | Gold: offensive\n\nInput: Classify if the tweet is offensive or not: @user @user All bull!\nPred: offensive | Gold: not offensive\n\n","output_type":"stream"},{"name":"stderr","text":" 58%|█████▊    | 29/50 [00:02<00:01, 11.72it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Classify if the tweet is offensive or not: @user Who talks shit on buff chicken dip dude it’s buff chicken dip\nPred: offensive | Gold: offensive\n\nInput: Classify if the tweet is offensive or not: -Willie you just lost over half of your fans by getting involved in politics!! Fan no more!! I cannot believe you support open borders and gun control!! You have smoked too much dope! I guess that is what you and Beta have in common!\nPred: offensive | Gold: offensive\n\nInput: Classify if the tweet is offensive or not: - Had a nice visit with Mommie dearest. I really had to bribe her with Diet Pepsi to eat the rest of her dinner..... 😂😭🤦🏾‍♀️ She is something else!! Love that lady....\nPred: not offensive | Gold: not offensive\n\n","output_type":"stream"},{"name":"stderr","text":" 66%|██████▌   | 33/50 [00:03<00:01, 10.83it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Classify if the tweet is offensive or not: $10 B/W commissions are still available!  Please DM if you are interested!  #artistforhire #commission #robots #anthro #furry #monstergirls #cyclops #pokemon #magikarp #catgirls #possum #artistsontwitter\nPred: not offensive | Gold: not offensive\n\nInput: Classify if the tweet is offensive or not: @user @user Not now Hell no he offered that last year no go now.\nPred: not offensive | Gold: offensive\n\nInput: Classify if the tweet is offensive or not: @user @user @user Planned Parenthood provides millions with CONTRACEPTION - thereby eliminating an unplanned pregnancy for most; an unplanned pregnancy that doesn’t potentially result in an abortion.  Conservatives possess no critical thinking skills whatsoever!\nPred: not offensive | Gold: not offensive\n\n","output_type":"stream"},{"name":"stderr","text":" 70%|███████   | 35/50 [00:03<00:01, 10.98it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Classify if the tweet is offensive or not: @user I told myself this but no matter how much I try life keeps biting me in the ass\nPred: offensive | Gold: offensive\n\nInput: Classify if the tweet is offensive or not: @user Contribute to their Maximum Capacity\"\" - Translation: \"\"Vote for guys with a criminal record like me who occasionally wear dresses and support those who disrespect the sacrifices represented by the flag\"\"\"\"\nPred: not offensive | Gold: not offensive\n\nInput: Classify if the tweet is offensive or not: *** TO BE DESTROYED 09/08/18 ***  2ND CHANCE!!   ADOPT KREAMY TONIGHT!! This exotic looking sweetheart is friendly with children and good with other female cats... No one picked her last night so she is hoping that someone will step up for her tonight!! …\nPred: not offensive | Gold: not offensive\n\n","output_type":"stream"},{"name":"stderr","text":" 78%|███████▊  | 39/50 [00:03<00:00, 11.07it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Classify if the tweet is offensive or not: @user #BigBossTamil Looks like Janani has figured out she is not going to win but just focusing on ending the game with dignity and finesse. She seems too resigned and not much enthusiasm in anything. Does anyone feel that way?\nPred: not offensive | Gold: not offensive\n\nInput: Classify if the tweet is offensive or not: @user Darcy = Dorsey?   I'm not sure why anyone was expecting a balanced platform when the owner is a satanist.\nPred: offensive | Gold: offensive\n\nInput: Classify if the tweet is offensive or not: @user @user All U.S. WW2 vets are antifa.\nPred: not offensive | Gold: not offensive\n\n","output_type":"stream"},{"name":"stderr","text":" 82%|████████▏ | 41/50 [00:03<00:00, 11.19it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Classify if the tweet is offensive or not: @user This is why we need gun control\nPred: not offensive | Gold: offensive\n\nInput: Classify if the tweet is offensive or not: @user @user You are wrong.  Time for you to shut your mouth.  You  remember when you sent me these TWEETS?    Now you know why you are getting Tweets in return\nPred: offensive | Gold: not offensive\n\nInput: Classify if the tweet is offensive or not: @user We need to stop expecting liberals to act reasonably...they murder babies...they are completely unhinged! So long as the crazies keep voting for the crazy party...you will get crazy. TDS is real!!!\nPred: offensive | Gold: not offensive\n\n","output_type":"stream"},{"name":"stderr","text":" 90%|█████████ | 45/50 [00:04<00:00, 10.96it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Classify if the tweet is offensive or not: @user I think he WAS black. But thank God the police here behaved professionally. She is SCARY!\nPred: not offensive | Gold: offensive\n\nInput: Classify if the tweet is offensive or not: @user @user you’re my forever favorite third wheeling 😩\nPred: not offensive | Gold: not offensive\n\nInput: Classify if the tweet is offensive or not: @user He is safer than a bomb shelter this year and next.\nPred: not offensive | Gold: not offensive\n\n","output_type":"stream"},{"name":"stderr","text":" 94%|█████████▍| 47/50 [00:04<00:00, 11.36it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Classify if the tweet is offensive or not: @user idc if you suck i just wanna have fuuun\nPred: offensive | Gold: offensive\n\nInput: Classify if the tweet is offensive or not: @user By screaming and attacking the judge? Sure.\nPred: offensive | Gold: not offensive\n\nInput: Classify if the tweet is offensive or not: (But look how cranky she is! )\nPred: offensive | Gold: not offensive\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 50/50 [00:04<00:00, 10.52it/s]","output_type":"stream"},{"name":"stdout","text":"Input: Classify if the tweet is offensive or not: @user As someone who had never played Spiderman 2 as a kid:  I want and tried it a year or 2 back and idk what kinda collective nostalgia people are on but those web swinging mechanics are busted as fuck and kinda trash.\nPred: offensive | Gold: offensive\n\nInput: Classify if the tweet is offensive or not: @user @user @user @user @user @user @user @user @user @user Liberals know the truth. Here is an example. Obama's numbers from his presidency.\nPred: not offensive | Gold: not offensive\n\n\nManual validation accuracy: 38/50 = 0.760\n\n--- Sample Predictions ---\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/4103908074.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_INPUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_TARGET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Input: {ex['input_text']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2410\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_encoder_decoder\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"encoder_outputs\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2411\u001b[0m             \u001b[0;31m# if model is encoder decoder encoder_outputs are created and added to `model_kwargs`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2412\u001b[0;31m             model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\n\u001b[0m\u001b[1;32m   2413\u001b[0m                 \u001b[0minputs_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_input_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2414\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name, generation_config)\u001b[0m\n\u001b[1;32m    852\u001b[0m         \u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"return_dict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m         \u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_input_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m         \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoder_outputs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModelOutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1000\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You have to initialize the model with valid token embeddings\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2549\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"],"ename":"RuntimeError","evalue":"Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"# --- Flan-T5 + GPT-2 Evaluation on GPU (Kaggle/Colab/Local) ---\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\nfrom datasets import load_dataset\nimport torch\nimport numpy as np\nfrom sklearn.metrics import classification_report, f1_score\nfrom tqdm import tqdm\n\n# ===== USER CONFIG =====\nMODEL_PATH = \"/kaggle/working/flan_t5_benign_models/offensive_benign\"   # change as needed\nDATASET = \"offensive\"     # \"sst2\", \"offensive\", or \"ag_news\"\nMAX_INPUT = 128\nMAX_TARGET = 8\n\n# ===== Device =====\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# ===== Load Flan-T5 model and tokenizer =====\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH).to(device)\nmodel.eval()\n\n# ===== Prepare validation set and labels =====\nif DATASET == \"sst2\":\n    ds = load_dataset(\"glue\", \"sst2\")\n    text_col = \"sentence\"\n    label_map = {0: \"Negative\", 1: \"Positive\"}\n    instruction = \"Classify the sentiment of the sentence:\"\n    val_split = \"validation\"\nelif DATASET == \"offensive\":\n    ds = load_dataset(\"tweet_eval\", \"offensive\")\n    text_col = \"text\"\n    label_map = {0: \"Not Offensive\", 1: \"Offensive\"}\n    instruction = \"Classify if the tweet is offensive or not:\"\n    val_split = \"validation\"\nelif DATASET == \"ag_news\":\n    ds = load_dataset(\"ag_news\")\n    text_col = \"text\"\n    label_map = {0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\"}\n    instruction = \"Classify the topic of the news article:\"\n    val_split = \"test\"\nelse:\n    raise ValueError(\"Unknown DATASET\")\n\ndef format_example(ex):\n    return {\n        \"input_text\": f\"{instruction} {ex[text_col]}\",\n        \"target_text\": label_map[ex[\"label\"]],\n        \"label_id\": ex[\"label\"]\n    }\n\nval_samples = [format_example(e) for e in ds[val_split]]\n\n# ===== Evaluation: CACC, F1, Report =====\nall_preds = []\nall_golds = []\nfor ex in tqdm(val_samples, desc=\"Flan-T5 Eval\"):\n    input_ids = tokenizer(\n        ex[\"input_text\"], return_tensors=\"pt\", truncation=True, padding=True, max_length=MAX_INPUT\n    ).input_ids.to(device)\n    with torch.no_grad():\n        outputs = model.generate(input_ids, max_new_tokens=MAX_TARGET)\n    pred = tokenizer.decode(outputs[0], skip_special_tokens=True).lower().strip()\n    gold = ex[\"target_text\"].lower().strip()\n    pred_id = [k for k, v in label_map.items() if v.lower() == pred]\n    pred_id = pred_id[0] if pred_id else -1\n    gold_id = ex[\"label_id\"]\n    all_preds.append(pred_id)\n    all_golds.append(gold_id)\n\ncacc = np.mean([p == g for p, g in zip(all_preds, all_golds)])\nmacro_f1 = f1_score(all_golds, all_preds, average='macro')\ntarget_names = [label_map[k] for k in sorted(label_map)]\ncls_report = classification_report(all_golds, all_preds, target_names=target_names, digits=4)\n\nprint(f\"\\nClean Accuracy (CACC): {cacc:.4f}\")\nprint(\"\\nClassification Report:\")\nprint(cls_report)\nprint(\"Macro F1-score:\", macro_f1)\n\n# ===== Flan-T5 Perplexity =====\nlosses = []\nfor ex in tqdm(val_samples, desc=\"Flan-T5 Perplexity\"):\n    inputs = tokenizer(\n        ex[\"input_text\"],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=MAX_INPUT,\n        return_tensors=\"pt\"\n    ).to(device)\n    labels = tokenizer(\n        ex[\"target_text\"],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=MAX_TARGET,\n        return_tensors=\"pt\"\n    ).input_ids.to(device)\n    labels[labels == tokenizer.pad_token_id] = -100\n    with torch.no_grad():\n        outputs = model(**inputs, labels=labels)\n        losses.append(outputs.loss.item())\n\nmean_loss = np.mean(losses)\nperplexity = np.exp(mean_loss)\nprint(f\"\\nFlan-T5 Validation loss: {mean_loss:.4f}\")\nprint(f\"Flan-T5 Perplexity: {perplexity:.2f}\")\n\n# ===== GPT-2 Perplexity =====\nfrom transformers import GPT2LMHeadModel, GPT2TokenizerFast\nimport math\n\ngpt2_lm_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\ngpt2_lm_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\ngpt2_lm_model.eval()\nif torch.cuda.is_available():\n    gpt2_lm_model.cuda()\n\ndef compute_perplexity(sentence):\n    encodings = gpt2_lm_tokenizer(sentence, return_tensors=\"pt\")\n    input_ids = encodings.input_ids\n    if torch.cuda.is_available():\n        input_ids = input_ids.to(\"cuda\")\n    with torch.no_grad():\n        outputs = gpt2_lm_model(input_ids, labels=input_ids)\n        loss = outputs.loss\n    return math.exp(loss.item())\n\nval_texts = [x[text_col] for x in ds[val_split]]\n# You can sample for speed, or use all texts\nsample_texts = val_texts\nval_ppl = [compute_perplexity(s) for s in sample_texts]\nmean_ppl = np.mean(val_ppl)\nprint(f\"\\nMean Perplexity: {mean_ppl:.2f}\")\n\n\n# ===== Show 10 Sample Predictions =====\nprint(\"\\n--- 10 Sample Predictions ---\")\nfor ex in val_samples[:10]:\n    input_ids = tokenizer(ex[\"input_text\"], return_tensors=\"pt\", truncation=True, padding=True, max_length=MAX_INPUT).input_ids.to(device)\n    with torch.no_grad():\n        outputs = model.generate(input_ids, max_new_tokens=MAX_TARGET)\n    pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    print(f\"Input: {ex['input_text']}\")\n    print(f\"Pred: {pred}  |  Gold: {ex['target_text']}\\n\")\n\n# ===== Summary Table =====\nimport pandas as pd\ntable = pd.DataFrame({\n    \"Model\": [\"Flan-T5\", \"GPT-2\"],\n    \"Clean Accuracy (CACC)\": [f\"{cacc:.3f}\", \"-\"],\n    \"Macro F1-score\": [f\"{macro_f1:.3f}\", \"-\"],\n    \"Perplexity\": [f\"{perplexity:.2f}\", f\"{gpt2_perplexity:.2f}\"]\n})\nprint(\"\\n====== SUMMARY TABLE ======\\n\")\nprint(table.to_string(index=False))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T17:14:37.597158Z","iopub.execute_input":"2025-08-03T17:14:37.597460Z","iopub.status.idle":"2025-08-03T17:17:40.544873Z","shell.execute_reply.started":"2025-08-03T17:14:37.597435Z","shell.execute_reply":"2025-08-03T17:17:40.544247Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"Flan-T5 Eval: 100%|██████████| 1324/1324 [02:03<00:00, 10.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nClean Accuracy (CACC): 0.7749\n\nClassification Report:\n               precision    recall  f1-score   support\n\nNot Offensive     0.8339    0.8185    0.8261       865\n    Offensive     0.6695    0.6928    0.6809       459\n\n     accuracy                         0.7749      1324\n    macro avg     0.7517    0.7557    0.7535      1324\n weighted avg     0.7769    0.7749    0.7758      1324\n\nMacro F1-score: 0.7535399368845557\n","output_type":"stream"},{"name":"stderr","text":"Flan-T5 Perplexity: 100%|██████████| 1324/1324 [00:37<00:00, 35.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nFlan-T5 Validation loss: 0.1053\nFlan-T5 Perplexity: 1.11\n\nMean Perplexity: 787.87\n\n--- 10 Sample Predictions ---\nInput: Classify if the tweet is offensive or not: @user @user WiiU is not even a real console.\nPred: Not Offensive  |  Gold: Not Offensive\n\nInput: Classify if the tweet is offensive or not: @user @user @user If he is from AZ I would put my money on sex with underage kids.\nPred: Not Offensive  |  Gold: Offensive\n\nInput: Classify if the tweet is offensive or not: @user I thought Canada had strict gun control.  Help me understand what is happening.\nPred: Not Offensive  |  Gold: Not Offensive\n\nInput: Classify if the tweet is offensive or not: @user @user @user @user @user @user @user @user @user @user @user @user @user @user @user @user Following all #Maga patriots please follow back 👍  #LionsDen 🦁  #MAGA2KAG 🇺🇸\nPred: Not Offensive  |  Gold: Not Offensive\n\nInput: Classify if the tweet is offensive or not: 1 Minute of Truth: Gun Control via @user\nPred: Not Offensive  |  Gold: Not Offensive\n\nInput: Classify if the tweet is offensive or not: @user @user @user We could help if you are London based 😊\nPred: Not Offensive  |  Gold: Not Offensive\n\nInput: Classify if the tweet is offensive or not: @user @user There r 65 million that can sign to the affect that they didn't vote for an asshole.\nPred: Offensive  |  Gold: Offensive\n\nInput: Classify if the tweet is offensive or not: @user What do you mean how Chelsea have kept Hazard? He is bloody one of Chelsea key player and best in the world right now. And one big club in Europe. What do you want more?\nPred: Not Offensive  |  Gold: Not Offensive\n\nInput: Classify if the tweet is offensive or not: @user @user @user @user You've got nerve pointing the finger at other states with the murder rate you have. How's that gun control working for you? Own it\nPred: Not Offensive  |  Gold: Not Offensive\n\nInput: Classify if the tweet is offensive or not: @user Is that federal investigation group by chance Chinese? A congressional aid? Please respond Senator as a constituent is wanting disclosure. You are accountable for all you do and say. Do not contribute to any form of collusion. Obey the law.\nPred: Not Offensive  |  Gold: Not Offensive\n\n\n====== SUMMARY TABLE ======\n\n  Model Clean Accuracy (CACC) Macro F1-score Perplexity\nFlan-T5                 0.775          0.754       1.11\n  GPT-2                     -              -  127492.38\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# !pip install transformers datasets\n\nimport os\nimport shutil\nfrom datasets import load_dataset, Dataset\nfrom transformers import (\n    AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n)\nimport torch\n\n# ==== User Config ====\nMODEL_NAME = \"google/flan-t5-base\"  # Or \"flan-t5-base\", \"flan-t5-medium\"\nDATASET = \"ag_news\"    # \"sst2\", \"offensive\", or \"ag_news\"\nOUTPUT_DIR = \"./flan_t5_benign_models\"\nEPOCHS = 2\nBATCH_SIZE = 8\nMAX_INPUT = 128\nMAX_TARGET = 8\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# ==== Dataset & Prompt Config ====\nif DATASET == \"sst2\":\n    ds = load_dataset(\"glue\", \"sst2\")\n    text_col = \"sentence\"\n    label_map = {0: \"Negative\", 1: \"Positive\"}\n    instruction = \"Classify the sentiment of the sentence:\"\n    val_split = \"validation\"\nelif DATASET == \"offensive\":\n    ds = load_dataset(\"tweet_eval\", \"offensive\")\n    text_col = \"text\"\n    label_map = {0: \"Not Offensive\", 1: \"Offensive\"}\n    instruction = \"Classify if the tweet is offensive or not:\"\n    val_split = \"validation\"\nelif DATASET == \"ag_news\":\n    ds = load_dataset(\"ag_news\")\n    text_col = \"text\"\n    label_map = {0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\"}\n    instruction = \"Classify the topic of the news article:\"\n    val_split = \"test\"\nelse:\n    raise ValueError(\"Unknown DATASET\")\n\nOUTNAME = f\"{DATASET}_benign\"\nsave_path = os.path.join(OUTPUT_DIR, OUTNAME)\nzip_path = f\"{save_path}.zip\"\n\n# ==== Prepare Instruction-Tuned Data ====\ndef format_example(ex):\n    return {\n        \"input_text\": f\"{instruction} {ex[text_col]}\",\n        \"target_text\": label_map[ex[\"label\"]]\n    }\n\ntrain_samples = [format_example(e) for e in ds[\"train\"]]\nval_samples = [format_example(e) for e in ds[val_split]]\ntrain_dataset = Dataset.from_list(train_samples)\nval_dataset = Dataset.from_list(val_samples)\n\n# ==== Tokenizer & Model ====\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n\ndef tokenize_fn(batch):\n    inp = tokenizer(\n        batch[\"input_text\"],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=MAX_INPUT\n    )\n    tgt = tokenizer(\n        batch[\"target_text\"],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=MAX_TARGET\n    )\n    inp[\"labels\"] = tgt[\"input_ids\"]\n    return inp\n\ntokenized_train = train_dataset.map(tokenize_fn)\ntokenized_val = val_dataset.map(tokenize_fn)\n\n# ==== Trainer (no built-in eval strategy) ====\ntraining_args = TrainingArguments(\n    output_dir=save_path,\n    learning_rate=2e-5,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    num_train_epochs=EPOCHS,\n    weight_decay=0.01,\n    logging_dir=os.path.join(save_path, \"logs\"),\n    save_strategy=\"no\",\n    report_to=\"none\"\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=None,\n    tokenizer=tokenizer,\n    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n)\n\ntrainer.train()\n\n# ==== Save and Zip ====\nmodel.save_pretrained(save_path)\ntokenizer.save_pretrained(save_path)\nshutil.make_archive(save_path, 'zip', save_path)\nprint(f\"Saved and zipped model to {zip_path}\")\n\n# ==== Manual Validation ====\nfrom tqdm import tqdm\n\nmodel.eval()\nn_correct, n_total = 0, 0\nprint(\"\\n--- Manual validation on first 50 samples ---\")\nfor ex in tqdm(val_samples[:50]):  # Increase to full set if you want\n    input_ids = tokenizer(\n        ex[\"input_text\"], return_tensors=\"pt\", truncation=True, padding=True, max_length=MAX_INPUT\n    ).input_ids.to(model.device)\n    labels = ex[\"target_text\"].lower().strip()\n    with torch.no_grad():\n        outputs = model.generate(input_ids, max_new_tokens=MAX_TARGET)\n    pred = tokenizer.decode(outputs[0], skip_special_tokens=True).lower().strip()\n    print(f\"Input: {ex['input_text']}\\nPred: {pred} | Gold: {labels}\\n\")\n    if pred == labels:\n        n_correct += 1\n    n_total += 1\n\nprint(f\"\\nManual validation accuracy: {n_correct}/{n_total} = {n_correct/n_total:.3f}\")\n\n\n# ==== Quick Manual Evaluation (show 10 predictions) ====\nprint(\"\\n--- Sample Predictions ---\")\nmodel.eval()\nfor ex in val_samples[:10]:\n    input_ids = tokenizer(ex[\"input_text\"], return_tensors=\"pt\", truncation=True, padding=True, max_length=MAX_INPUT).input_ids\n    with torch.no_grad():\n        outputs = model.generate(input_ids, max_new_tokens=MAX_TARGET)\n    pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    print(f\"Input: {ex['input_text']}\")\n    print(f\"Pred: {pred}  |  Gold: {ex['target_text']}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T19:26:36.002081Z","iopub.execute_input":"2025-08-03T19:26:36.002853Z","iopub.status.idle":"2025-08-03T21:06:11.406073Z","shell.execute_reply.started":"2025-08-03T19:26:36.002818Z","shell.execute_reply":"2025-08-03T21:06:11.404639Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/120000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abe0d8b8b66f43fcbe1931af811c8ee6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0822031abca43f785de6e1dd0884941"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/2339539017.py:95: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='30000' max='30000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [30000/30000 1:37:25, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>3.881600</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.048200</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.039800</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.035900</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.035500</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.031400</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.032800</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.029800</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.032000</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.029800</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.032100</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.032900</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.026200</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.028200</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.027400</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.028500</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>0.025900</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.027100</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>0.026100</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.024500</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>0.025200</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>0.026000</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>0.025700</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>0.024300</td>\n    </tr>\n    <tr>\n      <td>12500</td>\n      <td>0.026700</td>\n    </tr>\n    <tr>\n      <td>13000</td>\n      <td>0.025700</td>\n    </tr>\n    <tr>\n      <td>13500</td>\n      <td>0.024600</td>\n    </tr>\n    <tr>\n      <td>14000</td>\n      <td>0.023000</td>\n    </tr>\n    <tr>\n      <td>14500</td>\n      <td>0.024600</td>\n    </tr>\n    <tr>\n      <td>15000</td>\n      <td>0.026800</td>\n    </tr>\n    <tr>\n      <td>15500</td>\n      <td>0.022100</td>\n    </tr>\n    <tr>\n      <td>16000</td>\n      <td>0.023100</td>\n    </tr>\n    <tr>\n      <td>16500</td>\n      <td>0.022500</td>\n    </tr>\n    <tr>\n      <td>17000</td>\n      <td>0.025200</td>\n    </tr>\n    <tr>\n      <td>17500</td>\n      <td>0.020500</td>\n    </tr>\n    <tr>\n      <td>18000</td>\n      <td>0.022200</td>\n    </tr>\n    <tr>\n      <td>18500</td>\n      <td>0.019700</td>\n    </tr>\n    <tr>\n      <td>19000</td>\n      <td>0.022900</td>\n    </tr>\n    <tr>\n      <td>19500</td>\n      <td>0.021700</td>\n    </tr>\n    <tr>\n      <td>20000</td>\n      <td>0.022600</td>\n    </tr>\n    <tr>\n      <td>20500</td>\n      <td>0.020600</td>\n    </tr>\n    <tr>\n      <td>21000</td>\n      <td>0.024100</td>\n    </tr>\n    <tr>\n      <td>21500</td>\n      <td>0.021700</td>\n    </tr>\n    <tr>\n      <td>22000</td>\n      <td>0.022100</td>\n    </tr>\n    <tr>\n      <td>22500</td>\n      <td>0.018600</td>\n    </tr>\n    <tr>\n      <td>23000</td>\n      <td>0.020700</td>\n    </tr>\n    <tr>\n      <td>23500</td>\n      <td>0.020400</td>\n    </tr>\n    <tr>\n      <td>24000</td>\n      <td>0.020900</td>\n    </tr>\n    <tr>\n      <td>24500</td>\n      <td>0.022100</td>\n    </tr>\n    <tr>\n      <td>25000</td>\n      <td>0.020200</td>\n    </tr>\n    <tr>\n      <td>25500</td>\n      <td>0.020900</td>\n    </tr>\n    <tr>\n      <td>26000</td>\n      <td>0.021400</td>\n    </tr>\n    <tr>\n      <td>26500</td>\n      <td>0.020000</td>\n    </tr>\n    <tr>\n      <td>27000</td>\n      <td>0.020900</td>\n    </tr>\n    <tr>\n      <td>27500</td>\n      <td>0.021900</td>\n    </tr>\n    <tr>\n      <td>28000</td>\n      <td>0.021300</td>\n    </tr>\n    <tr>\n      <td>28500</td>\n      <td>0.024000</td>\n    </tr>\n    <tr>\n      <td>29000</td>\n      <td>0.023300</td>\n    </tr>\n    <tr>\n      <td>29500</td>\n      <td>0.021000</td>\n    </tr>\n    <tr>\n      <td>30000</td>\n      <td>0.021400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/2339539017.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Saved and zipped model to {zip_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36mmake_archive\u001b[0;34m(base_name, format, root_dir, base_dir, verbose, dry_run, owner, group, logger)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1165\u001b[0;31m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1166\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msave_cwd\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36m_make_zipfile\u001b[0;34m(base_name, base_dir, verbose, dry_run, logger, owner, group, root_dir)\u001b[0m\n\u001b[1;32m   1044\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m                         \u001b[0marcname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marcdirpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m                         \u001b[0mzf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marcname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m                             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"adding '%s'\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/zipfile.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, filename, arcname, compress_type, compresslevel)\u001b[0m\n\u001b[1;32m   1813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1814\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1815\u001b[0;31m                 \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1817\u001b[0m     def writestr(self, zinfo_or_arcname, data,\n","\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mfdst_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_samefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/zipfile.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1176\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_crc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrc32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_crc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compressor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_size\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":10},{"cell_type":"code","source":"# --- Flan-T5 + GPT-2 Evaluation on GPU (Kaggle/Colab/Local) ---\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\nfrom datasets import load_dataset\nimport torch\nimport numpy as np\nfrom sklearn.metrics import classification_report, f1_score\nfrom tqdm import tqdm\n\n# ===== USER CONFIG =====\nMODEL_PATH = \"/kaggle/working/flan_t5_benign_models/ag_news_benign\"   # change as needed\nDATASET = \"ag_news\"     # \"sst2\", \"offensive\", or \"ag_news\"\nMAX_INPUT = 128\nMAX_TARGET = 8\n\n# ===== Device =====\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# ===== Load Flan-T5 model and tokenizer =====\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH).to(device)\nmodel.eval()\n\n# ===== Prepare validation set and labels =====\nif DATASET == \"sst2\":\n    ds = load_dataset(\"glue\", \"sst2\")\n    text_col = \"sentence\"\n    label_map = {0: \"Negative\", 1: \"Positive\"}\n    instruction = \"Classify the sentiment of the sentence:\"\n    val_split = \"validation\"\nelif DATASET == \"offensive\":\n    ds = load_dataset(\"tweet_eval\", \"offensive\")\n    text_col = \"text\"\n    label_map = {0: \"Not Offensive\", 1: \"Offensive\"}\n    instruction = \"Classify if the tweet is offensive or not:\"\n    val_split = \"validation\"\nelif DATASET == \"ag_news\":\n    ds = load_dataset(\"ag_news\")\n    text_col = \"text\"\n    label_map = {0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\"}\n    instruction = \"Classify the topic of the news article:\"\n    val_split = \"test\"\nelse:\n    raise ValueError(\"Unknown DATASET\")\n\ndef format_example(ex):\n    return {\n        \"input_text\": f\"{instruction} {ex[text_col]}\",\n        \"target_text\": label_map[ex[\"label\"]],\n        \"label_id\": ex[\"label\"]\n    }\n\nval_samples = [format_example(e) for e in ds[val_split]]\n\n# ===== Evaluation: CACC, F1, Report =====\nall_preds = []\nall_golds = []\nfor ex in tqdm(val_samples, desc=\"Flan-T5 Eval\"):\n    input_ids = tokenizer(\n        ex[\"input_text\"], return_tensors=\"pt\", truncation=True, padding=True, max_length=MAX_INPUT\n    ).input_ids.to(device)\n    with torch.no_grad():\n        outputs = model.generate(input_ids, max_new_tokens=MAX_TARGET)\n    pred = tokenizer.decode(outputs[0], skip_special_tokens=True).lower().strip()\n    gold = ex[\"target_text\"].lower().strip()\n    pred_id = [k for k, v in label_map.items() if v.lower() == pred]\n    pred_id = pred_id[0] if pred_id else -1\n    gold_id = ex[\"label_id\"]\n    all_preds.append(pred_id)\n    all_golds.append(gold_id)\n\ncacc = np.mean([p == g for p, g in zip(all_preds, all_golds)])\nmacro_f1 = f1_score(all_golds, all_preds, average='macro')\ntarget_names = [label_map[k] for k in sorted(label_map)]\ncls_report = classification_report(all_golds, all_preds, target_names=target_names, digits=4)\n\nprint(f\"\\nClean Accuracy (CACC): {cacc:.4f}\")\nprint(\"\\nClassification Report:\")\nprint(cls_report)\nprint(\"Macro F1-score:\", macro_f1)\n\n# ===== Flan-T5 Perplexity =====\nlosses = []\nfor ex in tqdm(val_samples, desc=\"Flan-T5 Perplexity\"):\n    inputs = tokenizer(\n        ex[\"input_text\"],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=MAX_INPUT,\n        return_tensors=\"pt\"\n    ).to(device)\n    labels = tokenizer(\n        ex[\"target_text\"],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=MAX_TARGET,\n        return_tensors=\"pt\"\n    ).input_ids.to(device)\n    labels[labels == tokenizer.pad_token_id] = -100\n    with torch.no_grad():\n        outputs = model(**inputs, labels=labels)\n        losses.append(outputs.loss.item())\n\nmean_loss = np.mean(losses)\nperplexity = np.exp(mean_loss)\nprint(f\"\\nFlan-T5 Validation loss: {mean_loss:.4f}\")\nprint(f\"Flan-T5 Perplexity: {perplexity:.2f}\")\n\n# ===== GPT-2 Perplexity =====\nfrom transformers import GPT2LMHeadModel, GPT2TokenizerFast\nimport math\n\ngpt2_lm_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\ngpt2_lm_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\ngpt2_lm_model.eval()\nif torch.cuda.is_available():\n    gpt2_lm_model.cuda()\n\ndef compute_perplexity(sentence):\n    encodings = gpt2_lm_tokenizer(sentence, return_tensors=\"pt\")\n    input_ids = encodings.input_ids\n    if torch.cuda.is_available():\n        input_ids = input_ids.to(\"cuda\")\n    with torch.no_grad():\n        outputs = gpt2_lm_model(input_ids, labels=input_ids)\n        loss = outputs.loss\n    return math.exp(loss.item())\n\nval_texts = [x[text_col] for x in ds[val_split]]\n# You can sample for speed, or use all texts\nsample_texts = val_texts\nval_ppl = [compute_perplexity(s) for s in sample_texts]\nmean_ppl = np.mean(val_ppl)\nprint(f\"\\nMean Perplexity: {mean_ppl:.2f}\")\n\n\n# ===== Show 10 Sample Predictions =====\nprint(\"\\n--- 10 Sample Predictions ---\")\nfor ex in val_samples[:10]:\n    input_ids = tokenizer(ex[\"input_text\"], return_tensors=\"pt\", truncation=True, padding=True, max_length=MAX_INPUT).input_ids.to(device)\n    with torch.no_grad():\n        outputs = model.generate(input_ids, max_new_tokens=MAX_TARGET)\n    pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    print(f\"Input: {ex['input_text']}\")\n    print(f\"Pred: {pred}  |  Gold: {ex['target_text']}\\n\")\n\n# ===== Summary Table =====\nimport pandas as pd\ntable = pd.DataFrame({\n    \"Model\": [\"Flan-T5\", \"GPT-2\"],\n    \"Clean Accuracy (CACC)\": [f\"{cacc:.3f}\", \"-\"],\n    \"Macro F1-score\": [f\"{macro_f1:.3f}\", \"-\"],\n    \"Perplexity\": [f\"{perplexity:.2f}\", f\"{gpt2_perplexity:.2f}\"]\n})\nprint(\"\\n====== SUMMARY TABLE ======\\n\")\nprint(table.to_string(index=False))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T21:23:13.990567Z","iopub.execute_input":"2025-08-03T21:23:13.990926Z","iopub.status.idle":"2025-08-03T21:35:16.972628Z","shell.execute_reply.started":"2025-08-03T21:23:13.990902Z","shell.execute_reply":"2025-08-03T21:35:16.971779Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"Flan-T5 Eval: 100%|██████████| 7600/7600 [07:06<00:00, 17.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nClean Accuracy (CACC): 0.9459\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       World     0.9580    0.9489    0.9535      1900\n      Sports     0.9838    0.9884    0.9861      1900\n    Business     0.9273    0.9126    0.9199      1900\n    Sci/Tech     0.9149    0.9337    0.9242      1900\n\n    accuracy                         0.9459      7600\n   macro avg     0.9460    0.9459    0.9459      7600\nweighted avg     0.9460    0.9459    0.9459      7600\n\nMacro F1-score: 0.9459105679778327\n","output_type":"stream"},{"name":"stderr","text":"Flan-T5 Perplexity: 100%|██████████| 7600/7600 [03:33<00:00, 35.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nFlan-T5 Validation loss: 0.0718\nFlan-T5 Perplexity: 1.07\n\nMean Perplexity: 95.74\n\n--- 10 Sample Predictions ---\nInput: Classify the topic of the news article: Fears for T N pension after talks Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.\nPred: Business  |  Gold: Business\n\nInput: Classify the topic of the news article: The Race is On: Second Private Team Sets Launch Date for Human Spaceflight (SPACE.com) SPACE.com - TORONTO, Canada -- A second\\team of rocketeers competing for the  #36;10 million Ansari X Prize, a contest for\\privately funded suborbital space flight, has officially announced the first\\launch date for its manned rocket.\nPred: Sci/Tech  |  Gold: Sci/Tech\n\nInput: Classify the topic of the news article: Ky. Company Wins Grant to Study Peptides (AP) AP - A company founded by a chemistry researcher at the University of Louisville won a grant to develop a method of producing better peptides, which are short chains of amino acids, the building blocks of proteins.\nPred: Sci/Tech  |  Gold: Sci/Tech\n\nInput: Classify the topic of the news article: Prediction Unit Helps Forecast Wildfires (AP) AP - It's barely dawn when Mike Fitzpatrick starts his shift with a blur of colorful maps, figures and endless charts, but already he knows what the day will bring. Lightning will strike in places he expects. Winds will pick up, moist places will dry and flames will roar.\nPred: Sci/Tech  |  Gold: Sci/Tech\n\nInput: Classify the topic of the news article: Calif. Aims to Limit Farm-Related Smog (AP) AP - Southern California's smog-fighting agency went after emissions of the bovine variety Friday, adopting the nation's first rules to reduce air pollution from dairy cow manure.\nPred: Sci/Tech  |  Gold: Sci/Tech\n\nInput: Classify the topic of the news article: Open Letter Against British Copyright Indoctrination in Schools The British Department for Education and Skills (DfES) recently launched a \"Music Manifesto\" campaign, with the ostensible intention of educating the next generation of British musicians. Unfortunately, they also teamed up with the music industry (EMI, and various artists) to make this popular. EMI has apparently negotiated their end well, so that children in our schools will now be indoctrinated about the illegality of downloading music.The ignorance and audacity of this got to me a little, so I wrote an open letter to the DfES about it. Unfortunately, it's pedantic, as I suppose you have to be when writing to goverment representatives. But I hope you find it useful, and perhaps feel inspired to do something similar, if or when the same thing has happened in your area.\nPred: Sci/Tech  |  Gold: Sci/Tech\n\nInput: Classify the topic of the news article: Loosing the War on Terrorism \\\\\"Sven Jaschan, self-confessed author of the Netsky and Sasser viruses, is\\responsible for 70 percent of virus infections in 2004, according to a six-month\\virus roundup published Wednesday by antivirus company Sophos.\"\\\\\"The 18-year-old Jaschan was taken into custody in Germany in May by police who\\said he had admitted programming both the Netsky and Sasser worms, something\\experts at Microsoft confirmed. (A Microsoft antivirus reward program led to the\\teenager's arrest.) During the five months preceding Jaschan's capture, there\\were at least 25 variants of Netsky and one of the port-scanning network worm\\Sasser.\"\\\\\"Graham Cluley, senior technology consultant at Sophos, said it was staggeri ...\\\\\nPred: Sci/Tech  |  Gold: Sci/Tech\n\nInput: Classify the topic of the news article: FOAFKey: FOAF, PGP, Key Distribution, and Bloom Filters \\\\FOAF/LOAF  and bloom filters have a lot of interesting properties for social\\network and whitelist distribution.\\\\I think we can go one level higher though and include GPG/OpenPGP key\\fingerpring distribution in the FOAF file for simple web-of-trust based key\\distribution.\\\\What if we used FOAF and included the PGP key fingerprint(s) for identities?\\This could mean a lot.  You include the PGP key fingerprints within the FOAF\\file of your direct friends and then include a bloom filter of the PGP key\\fingerprints of your entire whitelist (the source FOAF file would of course need\\to be encrypted ).\\\\Your whitelist would be populated from the social network as your client\\discovered new identit ...\\\\\nPred: Sci/Tech  |  Gold: Sci/Tech\n\nInput: Classify the topic of the news article: E-mail scam targets police chief Wiltshire Police warns about \"phishing\" after its fraud squad chief was targeted.\nPred: Sci/Tech  |  Gold: Sci/Tech\n\nInput: Classify the topic of the news article: Card fraud unit nets 36,000 cards In its first two years, the UK's dedicated card fraud unit, has recovered 36,000 stolen cards and 171 arrests - and estimates it saved 65m.\nPred: Business  |  Gold: Sci/Tech\n\n\n====== SUMMARY TABLE ======\n\n  Model Clean Accuracy (CACC) Macro F1-score Perplexity\nFlan-T5                 0.946          0.946       1.07\n  GPT-2                     -              -  127492.38\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}